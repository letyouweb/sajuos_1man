import os
import json
import re
import hashlib
from pathlib import Path
from typing import Any, Dict, List, Tuple

# ===== ì„¤ì • =====
INPUT_DIR = r"D:\SajuOS_Data\3_SajuOS_RuleCards_JSON"
OUTPUT_FILE = r"D:\SajuOS_Data\sajuos_master_db.jsonl"
REPORT_FILE = r"D:\SajuOS_Data\sajuos_master_db_report.json"

# [ë¹„ì‹ë³„/ë¹„ì¸ìš©] ì œê±° ê·œì¹™ (í•„ìš”í•˜ë©´ ë” ì¶”ê°€)
CITE_PATTERN = re.compile(r"\[cite:\s*.*?\]", re.IGNORECASE)
NAME_BLOCKLIST = ["ì •ë™ì°¬"]  # í˜¹ì‹œ ë‚¨ì•„ìˆìœ¼ë©´ ì œê±°

def scrub_text(s: Any) -> str:
    if s is None:
        return ""
    s = str(s)
    s = CITE_PATTERN.sub("", s)
    for name in NAME_BLOCKLIST:
        s = s.replace(name, "")
    # ë¶ˆí•„ìš”í•œ ê³µë°± ì •ë¦¬
    s = re.sub(r"\n{3,}", "\n\n", s).strip()
    return s

def stable_id(card: Dict[str, Any]) -> str:
    # idê°€ ì—†ê±°ë‚˜ ë¹„ì •ìƒì¼ ë•Œ ìƒì„± (ë‚´ìš© ê¸°ë°˜ í•´ì‹œ)
    payload = {
        "topic": card.get("topic", ""),
        "trigger": card.get("trigger", {}),
        "mechanism": scrub_text(card.get("mechanism", "")),
        "interpretation": scrub_text(card.get("interpretation", "")),
        "action": scrub_text(card.get("action", "")),
    }
    raw = json.dumps(payload, ensure_ascii=False, sort_keys=True)
    h = hashlib.sha1(raw.encode("utf-8")).hexdigest()[:12]
    return f"RC-{h}"

def normalize_card(card: Dict[str, Any], source_file: str, source_path: str, source_title: str) -> Dict[str, Any]:
    # í•„ìˆ˜ ìŠ¤í‚¤ë§ˆ ê°•ì œ
    norm: Dict[str, Any] = {}
    norm["id"] = str(card.get("id") or "").strip() or stable_id(card)

    norm["topic"] = str(card.get("topic") or "GENERAL").strip() or "GENERAL"

    try:
        norm["priority"] = int(card.get("priority", 5))
    except:
        norm["priority"] = 5

    trigger = card.get("trigger", {})
    if isinstance(trigger, str):
        # triggerê°€ ë¬¸ìì—´ë¡œ ë“¤ì–´ì˜¨ ì¼€ì´ìŠ¤ ë°©ì–´
        try:
            trigger = json.loads(trigger)
        except:
            trigger = {"raw": trigger}
    if not isinstance(trigger, dict):
        trigger = {"raw": trigger}
    norm["trigger"] = trigger

    # ë³¸ë¬¸ í…ìŠ¤íŠ¸(ë¹„ì¸ìš©/ë¹„ì‹ë³„ ìŠ¤í¬ëŸ½ í¬í•¨)
    norm["mechanism"] = scrub_text(card.get("mechanism", ""))
    norm["interpretation"] = scrub_text(card.get("interpretation", ""))
    norm["action"] = scrub_text(card.get("action", ""))

    # ë¶€ê°€ í•„ë“œ
    tags = card.get("tags", [])
    if isinstance(tags, str):
        tags = [t.strip() for t in re.split(r"[,\s]+", tags) if t.strip()]
    if not isinstance(tags, list):
        tags = []
    norm["tags"] = tags

    cautions = card.get("cautions", [])
    if isinstance(cautions, str):
        cautions = [c.strip() for c in cautions.split("\n") if c.strip()]
    if not isinstance(cautions, list):
        cautions = []
    norm["cautions"] = [scrub_text(x) for x in cautions if scrub_text(x)]

    norm["source_file"] = source_file
    norm["source_path"] = source_path
    norm["source_title"] = source_title

    return norm

def iter_rulecards(data: Any) -> List[Dict[str, Any]]:
    # íŒŒì¼ êµ¬ì¡°ê°€ {rulecards:[...]} ë˜ëŠ” ê·¸ëƒ¥ [...] ì¸ ì¼€ì´ìŠ¤ ëŒ€ì‘
    if isinstance(data, dict) and isinstance(data.get("rulecards"), list):
        return data["rulecards"]
    if isinstance(data, list):
        return data
    return []

def build_master_db():
    seen_ids = set()
    total_files = 0
    total_cards = 0
    written = 0
    skipped_dup = 0
    skipped_bad = 0

    topic_count = {}
    priority_count = {}

    out_path = Path(OUTPUT_FILE)
    out_path.parent.mkdir(parents=True, exist_ok=True)

    with open(OUTPUT_FILE, "w", encoding="utf-8") as out:
        for p in sorted(Path(INPUT_DIR).rglob("*.json")):
            total_files += 1
            try:
                with open(p, "r", encoding="utf-8") as f:
                    data = json.load(f)
            except Exception as e:
                skipped_bad += 1
                continue

            source_title = ""
            if isinstance(data, dict):
                source_title = str(data.get("title") or data.get("name") or "").strip()
            if not source_title:
                source_title = p.stem

            cards = iter_rulecards(data)
            total_cards += len(cards)

            for card in cards:
                if not isinstance(card, dict):
                    skipped_bad += 1
                    continue

                norm = normalize_card(
                    card=card,
                    source_file=p.name,
                    source_path=str(p),
                    source_title=source_title
                )

                rid = norm["id"]
                if rid in seen_ids:
                    skipped_dup += 1
                    continue
                seen_ids.add(rid)

                out.write(json.dumps(norm, ensure_ascii=False) + "\n")
                written += 1

                t = norm["topic"]
                topic_count[t] = topic_count.get(t, 0) + 1
                pr = str(norm["priority"])
                priority_count[pr] = priority_count.get(pr, 0) + 1

    report = {
        "input_dir": INPUT_DIR,
        "output_file": OUTPUT_FILE,
        "total_files": total_files,
        "total_cards_found": total_cards,
        "written_records": written,
        "skipped_dup": skipped_dup,
        "skipped_bad": skipped_bad,
        "topic_count": dict(sorted(topic_count.items(), key=lambda x: -x[1])),
        "priority_count": dict(sorted(priority_count.items(), key=lambda x: int(x[0]))),
    }

    with open(REPORT_FILE, "w", encoding="utf-8") as f:
        json.dump(report, f, ensure_ascii=False, indent=2)

    print("ğŸ‰ í†µí•© ì™„ë£Œ!")
    print(f"- íŒŒì¼: {total_files}ê°œ")
    print(f"- ì¹´ë“œ ë°œê²¬: {total_cards}ê°œ")
    print(f"- ê¸°ë¡ë¨: {written}ê°œ")
    print(f"- ì¤‘ë³µ ìŠ¤í‚µ: {skipped_dup}ê°œ / íŒŒì† ìŠ¤í‚µ: {skipped_bad}ê°œ")
    print(f"- JSONL: {OUTPUT_FILE}")
    print(f"- ë¦¬í¬íŠ¸: {REPORT_FILE}")

if __name__ == "__main__":
    build_master_db()
