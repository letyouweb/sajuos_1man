"""
GPT Interpreter - Production Ready
- Chat Completions API only
- Detailed error logging for Railway
- Robust fallback handling
- ğŸ”¥ P0: Truth Anchor (í™˜ê° ë°©ì§€ ê°•í™”) ì ìš©
"""
import json
import logging
import random
import asyncio
import re
from typing import Optional, Dict, Any, Tuple, Set
from openai import AsyncOpenAI, APIError, RateLimitError, APIConnectionError, AuthenticationError
import httpx

from app.config import get_settings
from app.models.schemas import ConcernType, InterpretResponse
from app.rules.interpretation_rules import get_full_system_prompt
from app.services.openai_key import get_openai_api_key, key_fingerprint, key_tail

logger = logging.getLogger(__name__)

# P0: ì›êµ­ ê¸€ì í™˜ê° ë°©ì§€ìš© ê¸°ì¤€ ë°ì´í„°
_ALL_STEMS_BRANCHES: Set[str] = set(list("ê°‘ì„ë³‘ì •ë¬´ê¸°ê²½ì‹ ì„ê³„ìì¶•ì¸ë¬˜ì§„ì‚¬ì˜¤ë¯¸ì‹ ìœ ìˆ í•´"))

GUARDRAIL_ADDON = """
## Rules
1. No specific person names
2. Professional consulting tone
3. No lecture-style language
4. Use JSON output only
"""

class GptInterpreter:
    def __init__(self):
        self._client = None
    
    def _get_client(self) -> AsyncOpenAI:
        settings = get_settings()
        api_key = get_openai_api_key()
        return AsyncOpenAI(
            api_key=api_key,
            timeout=httpx.Timeout(float(settings.sajuos_timeout), connect=15.0),
            max_retries=0
        )

    def _build_truth_anchor(self, saju_data: Dict[str, Any]) -> str:
        """
        ğŸ”¥ P0: ë™ì  Truth Anchor
        - ì›êµ­(ë…„/ì›”/ì¼/ì‹œ)ì—ì„œ ë“±ì¥í•œ ì²œê°„/ì§€ì§€ ê¸€ìë§Œ í—ˆìš©
        - ê¸ˆì§€ ê¸€ì ëª©ë¡ì„ ëª…ì‹œí•˜ì—¬ í™˜ê°ì„ ì›ì²œ ë´‰ì‡„
        """
        saju_data = saju_data or {}
        y = saju_data.get("year_pillar") or ""
        m = saju_data.get("month_pillar") or ""
        d = saju_data.get("day_pillar") or ""
        h = saju_data.get("hour_pillar") or ""

        # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ê¸€ì ì¶”ì¶œ
        pillars = [p for p in [y, m, d, h] if isinstance(p, str) and p]
        joined = "".join(pillars)
        allowed = sorted(set([ch for ch in joined if ch in _ALL_STEMS_BRANCHES]))
        allowed_set = set(allowed)
        
        # ê¸ˆì§€ëœ ê¸€ì ëª©ë¡ ìƒì„±
        forbidden = sorted([ch for ch in _ALL_STEMS_BRANCHES if ch not in allowed_set])
        forbidden_preview = "".join(forbidden[:14]) + ("â€¦" if len(forbidden) > 14 else "")
        allowed_preview = "".join(allowed) if allowed else "(unknown)"

        # ì—”ì§„ í™•ì • ë°ì´í„° (ì‹­ì„±, ì˜¤í–‰, ê²©êµ­)
        summary = saju_data.get("saju_summary") or {}
        ten_present = summary.get("ten_gods_present") or saju_data.get("ten_gods_present") or []
        elements_count = summary.get("elements_count") or {}
        elements_present = [k for k, v in elements_count.items() if isinstance(v, (int, float)) and v > 0]
        allowed_structures = summary.get("allowed_structure_names") or []
        primary_structure = summary.get("primary_structure") or ""

        return f"""
## ğŸš¨ ZERO TOLERANCE RULES (ì ˆëŒ€ ì¤€ìˆ˜)
1) **í—ˆìš© ê¸€ìë§Œ ì–¸ê¸‰**: ì´ ì›êµ­ì—ì„œ ì–¸ê¸‰ ê°€ëŠ¥í•œ ì²œê°„/ì§€ì§€ = [{allowed_preview}] ë¿ì´ë‹¤.
2) **ê¸ˆì§€ ê¸€ì ì–¸ê¸‰ ê¸ˆì§€**: [{forbidden_preview}] ë° í—ˆìš© ë°– ê¸€ìëŠ” ì ˆëŒ€ ì–¸ê¸‰í•˜ì§€ ë§ˆë¼.
3) **ìƒìƒ ê¸ˆì§€**: ì§€ì¥ê°„/ìˆ¨ì€ ê¸€ì/ì¶”ë¡ ìœ¼ë¡œ "ìˆë‹¤"ê³  ë§í•˜ì§€ ë§ˆë¼.
4) **ì˜¤íƒ€ ê¸ˆì§€**: 'ê±¸ë¡ê²©' ì‚¬ìš© ê¸ˆì§€. (ê±´ë¡ê²©ìœ¼ë¡œ í‘œê¸°)
5) **ë°ì´í„° ì •í•©ì„±**: 
   - 'ìˆë‹¤'ê³  ë‹¨ì • ê°€ëŠ¥í•œ ì‹­ì„±: {', '.join(ten_present) if ten_present else '(none)'}
   - ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ” ì˜¤í–‰: {', '.join(elements_present) if elements_present else '(unknown)'}
   - í—ˆìš©ëœ ê²©êµ­: {', '.join(allowed_structures[:12]) if allowed_structures else '(unknown)'}
   - ìµœìš°ì„  ê²©êµ­: {primary_structure or '(unknown)'}
""".strip()

    async def _call_llm_json(self, system_prompt: str, user_prompt: str) -> Tuple[Dict[str, Any], int]:
        settings = get_settings()
        client = self._get_client()
        full_system = system_prompt + "\n\n" + GUARDRAIL_ADDON
        
        for attempt in range(settings.sajuos_max_retries):
            try:
                response = await client.chat.completions.create(
                    model=settings.openai_model,
                    messages=[
                        {"role": "system", "content": full_system},
                        {"role": "user", "content": user_prompt}
                    ],
                    max_tokens=settings.max_output_tokens,
                    temperature=0.3,
                    response_format={"type": "json_object"}
                )
                content = response.choices[0].message.content
                tokens_used = response.usage.total_tokens if response.usage else 0
                
                parsed = self._parse_json(content)
                if parsed:
                    return parsed, tokens_used
            except Exception as e:
                logger.error(f"[LLM] Error: {str(e)}")
                await asyncio.sleep(1.0)
        
        raise Exception("LLM call failed after retries")

    def _parse_json(self, content: str) -> Optional[Dict[str, Any]]:
        try:
            return json.loads(content)
        except:
            match = re.search(r'\{[\s\S]*\}', content)
            if match:
                try: return json.loads(match.group())
                except: pass
        return None

    async def interpret(self, saju_data: Dict[str, Any], name: str, gender: Optional[str], concern_type: ConcernType, question: str) -> InterpretResponse:
        settings = get_settings()
        try:
            system_prompt = get_full_system_prompt(concern_type)
            user_prompt = self._build_prompt(saju_data, name, gender, concern_type, question)
            data, tokens = await self._call_llm_json(system_prompt, user_prompt)
            result = self._build_result(data, name)
            result["model_used"] = settings.openai_model
            result["tokens_used"] = tokens
            return InterpretResponse(**result)
        except Exception as e:
            logger.error(f"[INTERPRET] Failed: {str(e)}")
            return self._fallback(name)

    def _build_prompt(self, saju_data: Dict, name: str, gender: Optional[str], concern_type: ConcernType, question: str) -> str:
        # ì‚¬ì£¼ ë°ì´í„° ì •ë¦¬
        y = self._get_pillar(saju_data, "year_pillar")
        m = self._get_pillar(saju_data, "month_pillar")
        d = self._get_pillar(saju_data, "day_pillar")
        h = self._get_pillar(saju_data, "hour_pillar") or "N/A"
        
        day_master = saju_data.get("day_master", d[0] if d else "")
        day_master_elem = saju_data.get("day_master_element", "")
        
        gender_text = "Male" if gender == "male" else "Female" if gender == "female" else "N/A"
        
        saju_summary = saju_data.get("saju_summary", {})
        summary_json = json.dumps(saju_summary, ensure_ascii=False, indent=2) if saju_summary else "{}"
        
        # ğŸ”¥ P0: Truth Anchor ìƒì„± (ì§€ì‹œì„œ ìˆœì„œ ì ìš©)
        truth_anchor = self._build_truth_anchor(saju_data)
        
        return f"""[User Info]
- Gender: {gender_text}
- Concern: {concern_type}
- Question: {question}

[Saju]
- Year: {y} / Month: {m} / Day: {d} / Hour: {h}

[Day Master]
- Stem: {day_master} / Element: {day_master_elem}

{truth_anchor}

[ğŸ”´ Ground Truth saju_summary - ì´ ë°ì´í„°ê°€ ì •ë‹µì´ë‹¤]
{summary_json}

Analyze and respond in JSON format."""

    def _get_pillar(self, data: Dict, key: str) -> str:
        pillar = data.get(key, "")
        if isinstance(pillar, dict): return pillar.get("ganji", str(pillar))
        return str(pillar)

    def _build_result(self, data: Dict[str, Any], name: str) -> Dict[str, Any]:
        return {
            "success": True,
            "summary": data.get("summary", "ë¶„ì„ ì™„ë£Œ"),
            "structure": data,
            "day_master_analysis": data.get("day_master_analysis", ""),
            "strengths": data.get("strengths", []),
            "risks": data.get("risks", []),
            "answer": data.get("answer", ""),
            "action_plan": data.get("action_plan", []),
            "lucky_periods": data.get("lucky_periods", []),
            "caution_periods": data.get("caution_periods", []),
            "lucky_elements": data.get("lucky_elements", {}),
            "blessing": data.get("blessing", f"{name}ë‹˜ì„ ì‘ì›í•©ë‹ˆë‹¤!"),
            "disclaimer": "ë³¸ ë¶„ì„ ê²°ê³¼ëŠ” ì°¸ê³ ìš©ì…ë‹ˆë‹¤."
        }

    def _fallback(self, name: str) -> InterpretResponse:
        return InterpretResponse(success=False, summary="Service error", blessing=f"{name}ë‹˜, ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")

gpt_interpreter = GptInterpreter()