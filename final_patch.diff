diff --git a/app/routers/reports.py b/app/routers/reports.py
index 0000000..1111111 100644
--- a/app/routers/reports.py
+++ b/app/routers/reports.py
@@ -1,0 +1,317 @@
+"""
+Reports API Router v14 - P0: section_id ë§¤í•‘ + íƒ­ ê°•ì œ + markdown ì¶”ì¶œ ìš°ì„ ìˆœìœ„ + ë¹„ì •ìƒ completed ê°•ë“±
+â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
+ğŸ”¥ P0 í•µì‹¬: ë°±ì—”ë“œ ID â†’ í”„ë¡ íŠ¸ ID ë§¤í•‘
+- exec â†’ business_climate
+- money â†’ cashflow
+- business â†’ market_product
+- team â†’ team_partnership
+- health â†’ owner_risk
+- calendar â†’ sprint_12m
+- sprint â†’ action_90d
+â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
+"""
+
+from __future__ import annotations
+
+import logging
+import uuid
+from typing import Any, Dict, List, Optional, Tuple
+
+from fastapi import APIRouter, HTTPException, Query
+from pydantic import BaseModel, EmailStr
+
+logger = logging.getLogger(__name__)
+router = APIRouter(prefix="/api/v1/reports", tags=["reports"])
+
+# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
+# P0: ì„¹ì…˜ ë§¤í•‘
+# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
+
+BACKEND_TO_FRONTEND = {
+    "exec": "business_climate",
+    "money": "cashflow",
+    "business": "market_product",
+    "team": "team_partnership",
+    "health": "owner_risk",
+    "calendar": "sprint_12m",
+    "sprint": "action_90d",
+}
+
+FRONTEND_TABS_ORDER = [
+    "business_climate",
+    "cashflow",
+    "market_product",
+    "team_partnership",
+    "owner_risk",
+    "sprint_12m",
+    "action_90d",
+]
+
+
+def map_to_frontend_id(section_id: str) -> str:
+    if not section_id:
+        return ""
+    sid = str(section_id).strip()
+    return BACKEND_TO_FRONTEND.get(sid, sid)
+
+
+def extract_markdown_from_section(section: Dict[str, Any]) -> str:
+    """P0: ì„¹ì…˜ markdown ì¶”ì¶œ (DB ì»¬ëŸ¼/í•˜ìœ„í˜¸í™˜ ëª¨ë‘ ì§€ì›)"""
+    # 0) DB canonical (ìµœìš°ì„ )
+    if section.get("body_markdown"):
+        return section["body_markdown"]
+    # 1) ê¸°ì¡´ markdown ì»¬ëŸ¼
+    if section.get("markdown"):
+        return section["markdown"]
+    # 2) content ì»¬ëŸ¼
+    if section.get("content"):
+        return section["content"]
+    # 3) raw_jsonì—ì„œ ì¶”ì¶œ
+    raw_json = section.get("raw_json") or {}
+    if raw_json.get("body_markdown"):
+        return raw_json["body_markdown"]
+    if raw_json.get("markdown"):
+        return raw_json["markdown"]
+    if raw_json.get("content"):
+        return raw_json["content"]
+    return ""
+
+
+def normalize_section(section: Dict[str, Any]) -> Dict[str, Any]:
+    backend_id = section.get("section_id") or section.get("id", "") or ""
+    frontend_id = map_to_frontend_id(backend_id)
+    raw_json = section.get("raw_json") or {}
+    markdown = extract_markdown_from_section(section)
+
+    status = section.get("status") or raw_json.get("status") or "unknown"
+
+    return {
+        "section_id": frontend_id,
+        "id": frontend_id,  # í”„ë¡ íŠ¸ í˜¸í™˜
+        "backend_id": backend_id,
+        "status": status,
+        "title": section.get("title") or raw_json.get("title") or "",
+        "markdown": markdown or "",
+        "raw_json": raw_json,
+        "created_at": section.get("created_at") or raw_json.get("created_at"),
+        "updated_at": section.get("updated_at") or raw_json.get("updated_at"),
+    }
+
+
+def ensure_all_sections(sections_raw: List[Dict[str, Any]], job_id: str, job_status: str = "") -> List[Dict[str, Any]]:
+    by_front_id: Dict[str, Dict[str, Any]] = {}
+    for s in sections_raw or []:
+        ns = normalize_section(s)
+        if ns.get("section_id"):
+            by_front_id[ns["section_id"]] = ns
+
+    # íƒ­ ê°•ì œ ìƒì„±: ì—†ìœ¼ë©´ placeholder
+    out: List[Dict[str, Any]] = []
+    for tab_id in FRONTEND_TABS_ORDER:
+        if tab_id in by_front_id:
+            out.append(by_front_id[tab_id])
+            continue
+        logger.info(f"[Reports] ì„¹ì…˜ ìƒì„±ì¤‘ placeholder: {tab_id} | job={job_id}")
+        out.append(
+            {
+                "section_id": tab_id,
+                "id": tab_id,
+                "backend_id": "",
+                "status": "running" if job_status not in ("failed", "completed") else job_status,
+                "title": "",
+                "markdown": "",
+                "raw_json": {"placeholder": True, "job_id": job_id, "section_id": tab_id},
+                "created_at": None,
+                "updated_at": None,
+            }
+        )
+    return out
+
+
+def build_full_markdown(sections_normalized: List[Dict[str, Any]], name: str, target_year: int) -> str:
+    parts = [f"# {name}ë‹˜ì˜ {target_year} ë¹„ì¦ˆë‹ˆìŠ¤ ë¦¬í¬íŠ¸\n"]
+    for s in sections_normalized:
+        title = s.get("title") or s.get("section_id")
+        md = (s.get("markdown") or "").strip()
+        if not md:
+            continue
+        parts.append(f"\n---\n\n## {title}\n\n{md}\n")
+    return "".join(parts).strip()
+
+
+# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
+# P0: Request ëª¨ë¸
+# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
+
+class ReportStartRequest(BaseModel):
+    email: EmailStr
+    name: str = "ê³ ê°"
+    saju_result: Optional[Dict[str, Any]] = None
+    year_pillar: Optional[str] = None
+    month_pillar: Optional[str] = None
+    day_pillar: Optional[str] = None
+    hour_pillar: Optional[str] = None
+    target_year: int = 2026
+    question: str = ""
+    concern_type: str = "career"
+    survey_data: Optional[Dict[str, Any]] = None
+
+    # ğŸ”¥ P0: ì„±ë³„/ìƒë…„ì›”ì¼ ì •ë³´ ì „ë‹¬
+    gender: Optional[str] = None  # female/male/ì—¬/ë‚¨ ë“±
+    birth_info: Optional[Dict[str, Any]] = None  # {year,month,day,hour,minute,...}
+
+
+# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
+# Supabase ì—°ê²° (í”„ë¡œì íŠ¸ ì½”ë“œì˜ ì„œë¹„ìŠ¤ì— ë§ì¶° êµ¬í˜„ë˜ì–´ ìˆì–´ì•¼ í•¨)
+# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
+
+def get_supabase():
+    # í”„ë¡œì íŠ¸ ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ìœ ì§€í•´ì•¼ í•¨.
+    from app.services.supabase_service import SupabaseService
+
+    return SupabaseService()
+
+
+@router.post("/start")
+async def start_report(payload: ReportStartRequest):
+    supabase = get_supabase()
+    if not supabase or not supabase.is_available():
+        raise HTTPException(status_code=503, detail="Supabase ë¯¸ì—°ê²°")
+
+    # input_jsonì„ ì›Œì»¤ë¡œ ì „ë‹¬ (report_workerê°€ input_json/input_data ëª¨ë‘ ëŒ€ì‘)
+    input_data = {
+        "name": payload.name,
+        "question": payload.question,
+        "concern_type": payload.concern_type,
+        "target_year": payload.target_year,
+        "survey_data": payload.survey_data,
+        "saju_result": payload.saju_result,
+        "year_pillar": payload.year_pillar,
+        "month_pillar": payload.month_pillar,
+        "day_pillar": payload.day_pillar,
+        "hour_pillar": payload.hour_pillar,
+        "gender": payload.gender,
+        "birth_info": payload.birth_info,
+        "email": str(payload.email),
+    }
+
+    job = await supabase.create_report_job(input_data=input_data)
+    if not job or not job.get("id"):
+        raise HTTPException(status_code=500, detail="Failed to create report job")
+
+    return {
+        "job_id": job["id"],
+        "public_token": job.get("public_token"),
+        "status": job.get("status", "queued"),
+    }
+
+
+@router.get("/view/{job_id}")
+async def view_report(job_id: str, token: str = Query(..., description="Access token")):
+    try:
+        uuid.UUID(job_id)
+    except ValueError:
+        raise HTTPException(status_code=400, detail=f"Invalid job_id: {job_id}")
+
+    supabase = get_supabase()
+    if not supabase or not supabase.is_available():
+        raise HTTPException(status_code=503, detail="Supabase ë¯¸ì—°ê²°")
+
+    is_valid, job = await supabase.verify_job_token(job_id, token)
+    if not is_valid or not job:
+        raise HTTPException(status_code=404, detail="Invalid token or job not found")
+
+    sections_raw = await supabase.get_sections_ordered(job_id)
+    sections_raw = [s for s in (sections_raw or []) if s.get("job_id") == job_id]
+
+    job_status = job.get("status", "")
+    sections_normalized = ensure_all_sections(sections_raw, job_id, job_status=job_status)
+
+    db_section_count = len([s for s in sections_raw if s.get("section_id")])
+    if job_status == "completed" and db_section_count == 0:
+        logger.error(f"[Reports] COMPLETEDì¸ë° DB ì„¹ì…˜ 0ê°œ: {job_id}")
+
+    # P0: completedì¸ë° ë¹ˆ ì„¹ì…˜ì´ ì„ì´ë©´(placeholder/ì €ì¥ ì‹¤íŒ¨) completedë¡œ ë³´ì—¬ì£¼ì§€ ì•ŠìŒ
+    display_status = job_status
+    display_error = job.get("error")
+    if job_status == "completed":
+        empty_sections = [
+            s.get("section_id")
+            for s in sections_normalized
+            if len((s.get("markdown") or "").strip()) < 100
+        ]
+        if empty_sections:
+            display_status = "failed"
+            display_error = f"EMPTY_SECTIONS: {empty_sections}"
+            logger.error(f"[Reports] ë¹„ì •ìƒ ì™„ë£Œ ë¦¬í¬íŠ¸ ê°•ë“± ì²˜ë¦¬: {job_id} | {display_error}")
+
+    input_json = job.get("input_json") or {}
+    name = input_json.get("name", "ê³ ê°")
+    target_year = input_json.get("target_year", 2026)
+    full_markdown = build_full_markdown(sections_normalized, name, target_year)
+
+    logger.info(f"[Reports] view_report: {job_id} | db_sections={db_section_count} | total_tabs=7")
+
+    return {
+        "job": {
+            "id": job.get("id", job_id),
+            "status": display_status,
+            "progress": job.get("progress", 0),
+            "error": display_error,
+            "created_at": job.get("created_at"),
+            "completed_at": job.get("completed_at"),
+        },
+        "sections": sections_normalized,
+        "full_markdown": full_markdown,
+        "section_count": len(FRONTEND_TABS_ORDER),
+    }
+
+
+@router.get("/verify/{job_id}")
+async def verify_token(job_id: str, token: str = Query(...)):
+    try:
+        uuid.UUID(job_id)
+    except ValueError:
+        raise HTTPException(status_code=400, detail=f"Invalid job_id: {job_id}")
+
+    supabase = get_supabase()
+    if not supabase or not supabase.is_available():
+        raise HTTPException(status_code=503, detail="Supabase ë¯¸ì—°ê²°")
+
+    is_valid, job = await supabase.verify_job_token(job_id, token)
+    if not is_valid:
+        raise HTTPException(status_code=403, detail="Invalid token")
+
+    return {"valid": True, "job_id": job.get("id", job_id), "status": job.get("status")}
+
+
+@router.get("/{job_id}/status")
+async def get_report_status(job_id: str, token: Optional[str] = Query(None)):
+    """í”„ë¡ íŠ¸ í´ë§ìš©: job + ì„¹ì…˜ ì§„í–‰ë¥ """
+    try:
+        uuid.UUID(job_id)
+    except ValueError:
+        raise HTTPException(status_code=400, detail=f"Invalid job_id: {job_id}")
+
+    supabase = get_supabase()
+    if not supabase or not supabase.is_available():
+        raise HTTPException(status_code=503, detail="Supabase ë¯¸ì—°ê²°")
+
+    job = await supabase.get_report_job(job_id, token=token)
+    if not job:
+        raise HTTPException(status_code=404, detail="Job not found")
+
+    sections_raw = await supabase.get_sections_ordered(job_id)
+    sections_raw = [s for s in (sections_raw or []) if s.get("job_id") == job_id]
+    sections_normalized = ensure_all_sections(sections_raw, job_id, job_status=job.get("status", ""))
+
+    completed = len([s for s in sections_normalized if (s.get("status") in ("completed", "done", "success")) and (s.get("markdown") or "").strip()])
+    progress = max(job.get("progress", 0), int((completed / len(FRONTEND_TABS_ORDER)) * 100))
+
+    return {
+        "job_id": job_id,
+        "status": job.get("status"),
+        "progress": progress,
+        "sections": [{"section_id": s["section_id"], "status": s.get("status")} for s in sections_normalized],
+    }

diff --git a/app/services/report_worker.py b/app/services/report_worker.py
index 0000000..1111111 100644
--- a/app/services/report_worker.py
+++ b/app/services/report_worker.py
@@ -1,0 +1,853 @@
+"""
+Report Worker v13 - P0 Pivot: ì„¤ë¬¸ ê¸°ë°˜ RuleCardScorer í†µí•©
+â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
+ğŸ”¥ P0 í•µì‹¬ ë³€ê²½:
+1) _select_rulecards() â†’ RuleCardScorer.score_cards_for_section() í˜¸ì¶œ
+2) survey_dataê°€ ì¹´ë“œ ì„ íƒì— ì§ì ‘ ë°˜ì˜
+3) ê°™ì€ ì‚¬ì£¼ë¼ë„ ì„¤ë¬¸ì— ë”°ë¼ ë‹¤ë¥¸ ì¹´ë“œê°€ ì„ íƒë¨
+4) ì„¹ì…˜ë³„ score_trace ì €ì¥
+â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
+"""
+import asyncio
+import logging
+import time
+from datetime import date
+from typing import Dict, Any, Optional, List
+
+from app.services.supabase_service import supabase_service
+from app.services.saju_engine import calc_daeun_pillars
+
+logger = logging.getLogger(__name__)
+
+
+# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
+# ğŸ”¥ P0: ëŒ€ìš´ ê³„ì‚° í—¬í¼ í•¨ìˆ˜
+# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
+
+def _year_stem_is_yang(stem_ko: str) -> bool:
+    """ë…„ê°„ì´ ì–‘ê°„ì¸ì§€ í™•ì¸ (ê°‘ë³‘ë¬´ê²½ì„)"""
+    return stem_ko in ["ê°‘", "ë³‘", "ë¬´", "ê²½", "ì„"]
+
+
+def _normalize_gender(g: str) -> str:
+    """ì„±ë³„ ì •ê·œí™”"""
+    if not g:
+        return ""
+    g = str(g).strip().lower()
+    if g in ["female", "f", "ì—¬", "ì—¬ì", "ì—¬ì„±"]:
+        return "female"
+    if g in ["male", "m", "ë‚¨", "ë‚¨ì", "ë‚¨ì„±"]:
+        return "male"
+    return g
+
+
+def _calc_age(birth_info: dict) -> int:
+    """ìƒë…„ì›”ì¼ë¡œ ë§Œ ë‚˜ì´ ê³„ì‚°"""
+    if not birth_info:
+        return 0
+    y = birth_info.get("year")
+    m = birth_info.get("month", 1)
+    d = birth_info.get("day", 1)
+    if not y:
+        return 0
+    try:
+        today = date.today()
+        age = today.year - int(y)
+        if (today.month, today.day) < (int(m), int(d)):
+            age -= 1
+        return max(age, 0)
+    except:
+        return 0
+
+
+
+
+# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
+# ğŸ”¥ P0: ì›êµ­ íŒ©íŠ¸(ì‹­ì„±/ì˜¤í–‰) í™•ì • ìœ í‹¸
+# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
+
+STEM_ELEM_POLAR = {
+    "ê°‘": ("wood", "yang"), "ì„": ("wood", "yin"),
+    "ë³‘": ("fire", "yang"), "ì •": ("fire", "yin"),
+    "ë¬´": ("earth", "yang"), "ê¸°": ("earth", "yin"),
+    "ê²½": ("metal", "yang"), "ì‹ ": ("metal", "yin"),
+    "ì„": ("water", "yang"), "ê³„": ("water", "yin"),
+}
+
+BRANCH_HIDDEN_STEMS = {
+    "ì": ["ì„", "ê³„"],
+    "ì¶•": ["ê¸°", "ê³„", "ì‹ "],
+    "ì¸": ["ê°‘", "ë³‘", "ë¬´"],
+    "ë¬˜": ["ì„"],
+    "ì§„": ["ë¬´", "ì„", "ê³„"],
+    "ì‚¬": ["ë³‘", "ë¬´", "ê²½"],
+    "ì˜¤": ["ì •", "ê¸°"],
+    "ë¯¸": ["ê¸°", "ì •", "ì„"],
+    "ì‹ ": ["ê²½", "ì„", "ë¬´"],
+    "ìœ ": ["ì‹ "],
+    "ìˆ ": ["ë¬´", "ì‹ ", "ì •"],
+    "í•´": ["ê°‘", "ì„"],
+}
+
+GENERATOR = {"wood": "fire", "fire": "earth", "earth": "metal", "metal": "water", "water": "wood"}
+CONTROLS = {"wood": "earth", "earth": "water", "water": "fire", "fire": "metal", "metal": "wood"}
+
+
+def _pillar_parts(p: str):
+    if not p or len(p) < 2:
+        return ("", "")
+    return (p[0], p[1])
+
+
+def _ten_god(day_stem: str, other_stem: str) -> str:
+    """ì¼ê°„ ê¸°ì¤€ ì‹­ì„± ê³„ì‚°(ì²œê°„/ì§€ì¥ê°„ ê³µìš©)"""
+    if not day_stem or not other_stem:
+        return ""
+    dm = STEM_ELEM_POLAR.get(day_stem)
+    ot = STEM_ELEM_POLAR.get(other_stem)
+    if not dm or not ot:
+        return ""
+    dm_elem, dm_pol = dm
+    ot_elem, ot_pol = ot
+
+    # ë¹„ê²(ë™ì¼ ì˜¤í–‰)
+    if ot_elem == dm_elem:
+        return "ë¹„ê²¬" if ot_pol == dm_pol else "ê²ì¬"
+    # ì‹ìƒ(ë‚´ê°€ ìƒ)
+    if GENERATOR[dm_elem] == ot_elem:
+        return "ì‹ì‹ " if ot_pol == dm_pol else "ìƒê´€"
+    # ì¬ì„±(ë‚´ê°€ ê·¹)
+    if CONTROLS[dm_elem] == ot_elem:
+        return "í¸ì¬" if ot_pol == dm_pol else "ì •ì¬"
+    # ê´€ì„±(ë‚˜ë¥¼ ê·¹)
+    if CONTROLS[ot_elem] == dm_elem:
+        return "í¸ê´€" if ot_pol == dm_pol else "ì •ê´€"
+    # ì¸ì„±(ë‚˜ë¥¼ ìƒ)
+    if GENERATOR[ot_elem] == dm_elem:
+        return "í¸ì¸" if ot_pol == dm_pol else "ì •ì¸"
+    return ""
+
+
+def _compute_fact_flags(saju_data: dict) -> dict:
+    """ì›êµ­ 4ì£¼(ì²œê°„+ì§€ì¥ê°„) ê¸°ì¤€: ì‹­ì„±/ì˜¤í–‰/ì¬ì„± ìœ ë¬´ í™•ì •"""
+    yp = saju_data.get("year_pillar", "") or ""
+    mp = saju_data.get("month_pillar", "") or ""
+    dp = saju_data.get("day_pillar", "") or ""
+    hp = saju_data.get("hour_pillar", "") or ""
+    dm = saju_data.get("day_master", "") or ""
+
+    stems_all: List[str] = []
+    for p in [yp, mp, dp, hp]:
+        st, br = _pillar_parts(p)
+        if st:
+            stems_all.append(st)
+        if br and br in BRANCH_HIDDEN_STEMS:
+            stems_all.extend(BRANCH_HIDDEN_STEMS[br])
+
+    ten_gods: List[str] = []
+    elems: List[str] = []
+    for st in stems_all:
+        tg = _ten_god(dm, st)
+        if tg:
+            ten_gods.append(tg)
+        ep = STEM_ELEM_POLAR.get(st)
+        if ep:
+            elems.append(ep[0])
+
+    ten_gods_present = sorted(set(ten_gods))
+    elements_present = sorted(set(elems))
+    has_wealth_star = any(x in ten_gods_present for x in ["ì •ì¬", "í¸ì¬"])
+    has_robwealth = "ê²ì¬" in ten_gods_present
+
+    return {
+        "stems_all": stems_all,
+        "ten_gods_present": ten_gods_present,
+        "elements_present": elements_present,
+        "has_wealth_star": has_wealth_star,
+        "has_robwealth": has_robwealth,
+    }
+
+# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
+# ğŸ”¥ğŸ”¥ğŸ”¥ P0: 1ì¸ ìì˜ì—…ììš© ì„¹ì…˜ ìŠ¤í™ (ìƒˆ ID ë§¤í•‘)
+# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
+
+ONEMAN_SECTION_SPECS = [
+    {"id": "exec", "title": "ğŸŒ¦ï¸ 2026 ë¹„ì¦ˆë‹ˆìŠ¤ ì „ëµ ê¸°ìƒë„", "order": 1},
+    {"id": "money", "title": "ğŸ’° ìë³¸ ìœ ë™ì„± ë° í˜„ê¸ˆíë¦„ ìµœì í™”", "order": 2},
+    {"id": "business", "title": "ğŸ“ ì‹œì¥ í¬ì§€ì…”ë‹ ë° ìƒí’ˆ í™•ì¥ ì „ëµ", "order": 3},
+    {"id": "team", "title": "ğŸ¤ ì¡°ì§ í™•ì¥ ë° íŒŒíŠ¸ë„ˆì‹­ ê°€ì´ë“œ", "order": 4},
+    {"id": "health", "title": "ğŸ§¯ ì˜¤ë„ˆ ë¦¬ìŠ¤í¬ ê´€ë¦¬ ë° ë²ˆì•„ì›ƒ ë°©ì–´", "order": 5},
+    {"id": "calendar", "title": "ğŸ—“ï¸ 12ê°œì›” ë¹„ì¦ˆë‹ˆìŠ¤ ìŠ¤í”„ë¦°íŠ¸ ìº˜ë¦°ë”", "order": 6},
+    {"id": "sprint", "title": "ğŸš€ í–¥í›„ 90ì¼ ë§¤ì¶œ ê·¹ëŒ€í™” ì•¡ì…˜í”Œëœ", "order": 7},
+]
+
+
+class ReportWorker:
+    """ë°±ê·¸ë¼ìš´ë“œ ë¦¬í¬íŠ¸ ìƒì„± ì›Œì»¤ - P0 Pivot"""
+    
+    _running_jobs: set = set()
+    
+    async def run_job(self, job_id: str, rulestore: Any = None) -> None:
+        """Job ì‹¤í–‰"""
+        if job_id in self._running_jobs:
+            logger.warning(f"[Worker] ì´ë¯¸ ì‹¤í–‰ ì¤‘: {job_id}")
+            return
+        
+        self._running_jobs.add(job_id)
+        start_time = time.time()
+        
+        if rulestore:
+            card_count = len(getattr(rulestore, 'cards', [])) if hasattr(rulestore, 'cards') else 0
+            logger.info(f"[Worker] RuleStore ìˆ˜ì‹ : total={card_count}ì¥")
+        else:
+            logger.warning(f"[Worker] âš ï¸ RuleStoreê°€ None!")
+        
+        try:
+            success, error_msg = await self._execute_job(job_id, rulestore)
+            elapsed = int((time.time() - start_time) * 1000)
+            
+            if success:
+                logger.info(f"[Worker] âœ… Job ì™„ë£Œ: {job_id} ({elapsed}ms)")
+            else:
+                logger.error(f"[Worker] âŒ Job ì‹¤íŒ¨: {job_id} | {error_msg}")
+            
+        except Exception as e:
+            logger.error(f"[Worker] âŒ Job ì‹¤íŒ¨: {job_id} | {e}")
+            try:
+                await supabase_service.fail_job(job_id, str(e)[:500])
+            except:
+                pass
+            
+            try:
+                job = await supabase_service.get_job(job_id)
+                if job:
+                    await self._send_failure_email(job, str(e))
+            except Exception as email_err:
+                logger.warning(f"[Worker] ì‹¤íŒ¨ ì´ë©”ì¼ ë°œì†¡ ì‹¤íŒ¨: {email_err}")
+        
+        finally:
+            self._running_jobs.discard(job_id)
+    
+    async def _execute_job(self, job_id: str, rulestore: Any = None) -> tuple[bool, str]:
+        """ì‹¤ì œ Job ì‹¤í–‰"""
+        job = await supabase_service.get_job(job_id)
+        if not job:
+            raise ValueError(f"Job ì—†ìŒ: {job_id}")
+        
+        email = job.get("user_email", "")
+        input_json = job.get("input_json") or {}
+        
+        name = input_json.get("name", "ê³ ê°")
+        target_year = input_json.get("target_year", 2026)
+        question = input_json.get("question", "")
+        survey_data = input_json.get("survey_data") or {}
+        
+        await supabase_service.update_progress(job_id, 5, "running")
+        
+        # ğŸ”¥ P0: ì‚¬ì£¼ ë°ì´í„° ì¶”ì¶œ
+        saju_data = self._prepare_saju_data(input_json)
+        
+        # ğŸ”¥ğŸ”¥ğŸ”¥ P0 í•µì‹¬: ì‚¬ì£¼ ë°ì´í„° ë¬´ê²°ì„± ì²´í¬ - ë¹„ì–´ìˆìœ¼ë©´ ì—ëŸ¬!
+        missing_pillars = []
+        for key in ["year_pillar", "month_pillar", "day_pillar"]:
+            if not saju_data.get(key):
+                missing_pillars.append(key)
+        
+        if missing_pillars:
+            error_msg = f"ì‚¬ì£¼ ë°ì´í„° ëˆ„ë½: {missing_pillars}. ì‚¬ì£¼ ì—†ëŠ” ì‚¬ì£¼ ë¦¬í¬íŠ¸ëŠ” ìƒí’ˆ ê°€ì¹˜ê°€ ì—†ìŠµë‹ˆë‹¤."
+            logger.error(f"[Worker] âŒâŒâŒ {error_msg}")
+            logger.error(f"[Worker] input_json keys: {list(input_json.keys())}")
+            logger.error(f"[Worker] saju_result: {str(input_json.get('saju_result', {}))[:200] if input_json.get('saju_result') else 'None'}")
+            
+            # ğŸ”¥ P0: ì‚¬ì£¼ ë°ì´í„° ì—†ìœ¼ë©´ ì¦‰ì‹œ ì‹¤íŒ¨ ì²˜ë¦¬
+            await supabase_service.fail_job(job_id, error_msg)
+            return False, error_msg
+        
+        logger.info(f"[Worker] âœ… ì‚¬ì£¼ ê²€ì¦ í†µê³¼: {saju_data['year_pillar']}/{saju_data['month_pillar']}/{saju_data['day_pillar']}/{saju_data.get('hour_pillar', '-')}")
+        logger.info(f"[Worker] âœ… ì¼ê°„: {saju_data.get('day_master', '-')} ({saju_data.get('day_master_element', '-')})")
+        logger.info(f"[Worker] âœ… ìƒë…„ì›”ì¼ì‹œ: {saju_data.get('birth_info', '-')}")
+        
+        # ğŸ”¥ P0: Feature Tags ìƒì„±
+        feature_tags = self._build_feature_tags(saju_data)
+        
+        # ğŸ”¥ğŸ”¥ğŸ”¥ P0 í•µì‹¬: RuleCardScorerë¡œ ì„¤ë¬¸ ê¸°ë°˜ ì¹´ë“œ ì„ íƒ
+        all_cards = self._get_all_cards_as_dict(rulestore)
+        
+        logger.info(f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
+        logger.info(f"ğŸ”¥ [Worker] ì„¤ë¬¸ ê¸°ë°˜ RuleCard ìŠ¤ì½”ì–´ë§ ì‹œì‘")
+        logger.info(f"   survey_data: industry={survey_data.get('industry', '-')}, painPoint={survey_data.get('painPoint', '-')}, goal={survey_data.get('goal', '-')[:30] if survey_data.get('goal') else '-'}")
+        logger.info(f"   feature_tags: {len(feature_tags)}ê°œ")
+        logger.info(f"   ì „ì²´ ì¹´ë“œ: {len(all_cards)}ì¥")
+        logger.info(f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
+        
+        sections_result = {}
+        failed_sections = []
+        total_sections = len(ONEMAN_SECTION_SPECS)
+        all_used_card_ids = []
+        used_ids: set = set()
+        section_match_summaries = {}
+        
+        for idx, spec in enumerate(ONEMAN_SECTION_SPECS):
+            section_id = spec["id"]
+            section_title = spec["title"]
+            
+            progress = int((idx / total_sections) * 90) + 10
+            await supabase_service.update_progress(job_id, progress, "running")
+            
+            try:
+                # ğŸ”¥ğŸ”¥ğŸ”¥ P0 í•µì‹¬: ì„¹ì…˜ë³„ë¡œ RuleCardScorer í˜¸ì¶œ
+                section_cards, match_summary = self._select_rulecards_for_section(
+                    all_cards=all_cards,
+                    section_id=section_id,
+                    feature_tags=feature_tags,
+                    survey_data=survey_data,
+                    saju_data=saju_data,
+                    used_ids=used_ids,
+                )
+                
+                section_match_summaries[section_id] = match_summary
+                
+                # ì‚¬ìš©ëœ ì¹´ë“œ ID ìˆ˜ì§‘
+                # ì¤‘ë³µ ë°©ì§€ìš© set
+                for card in section_cards:
+                    if card.get('id'):
+                        used_ids.add(card['id'])
+                for card in section_cards[:10]:
+                    if card.get("id") and card["id"] not in all_used_card_ids:
+                        all_used_card_ids.append(card["id"])
+                
+                logger.info(f"[Worker:Section:{section_id}] ì¹´ë“œ ì„ íƒ ì™„ë£Œ: {len(section_cards)}ì¥ | AvgScore={match_summary.get('avg_score', 0):.1f}")
+                
+                # ì„¹ì…˜ ìƒì„±
+                section_result = await self._generate_section(
+                    section_id=section_id,
+                    section_title=section_title,
+                    saju_data=saju_data,
+                    rulecards=section_cards,
+                    feature_tags=feature_tags,
+                    target_year=target_year,
+                    question=question,
+                    survey_data=survey_data,
+                    match_summary=match_summary
+                )
+                
+                content = section_result.get("content", {})
+                ok = section_result.get("ok", True)
+                errors = section_result.get("guardrail_errors", [])
+                
+                body_markdown = content.get("body_markdown", "")
+                # ğŸ”¥ P0-C: ë¹ˆ ì„¹ì…˜ ì €ì¥ ì ˆëŒ€ ê¸ˆì§€ - ìµœì†Œ 300ì ë³´ì¥
+                if not body_markdown or len(body_markdown) < 300:
+                    logger.error(f"[Worker] âš ï¸ ì„¹ì…˜ ë³¸ë¬¸ ë¶€ì¡±: {section_id} | length={len(body_markdown)}")
+                    # ì—ëŸ¬ í…ìŠ¤íŠ¸ë¡œ ì±„ìš°ê¸°
+                    fallback_text = f"""## {section_title}
+
+ì´ ì„¹ì…˜ì˜ ë¶„ì„ ê²°ê³¼ë¥¼ ìƒì„±í•˜ëŠ” ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.
+
+### ì„ì‹œ ì•ˆë‚´
+- í˜„ì¬ ì‚¬ì£¼ ë¶„ì„ ì—”ì§„ì´ í•´ë‹¹ ì„¹ì…˜ì— ëŒ€í•œ ì¶©ë¶„í•œ ë°ì´í„°ë¥¼ í™•ë³´í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.
+- ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì‹œê±°ë‚˜, ê³ ê°ì„¼í„°ë¡œ ë¬¸ì˜í•´ ì£¼ì„¸ìš”.
+- ë¶„ì„ì— ì‚¬ìš©ëœ ë£°ì¹´ë“œ: {len(section_cards)}ì¥
+
+### ë‹¤ìŒ ë‹¨ê³„
+1. í˜ì´ì§€ë¥¼ ìƒˆë¡œê³ ì¹¨í•´ ë³´ì„¸ìš”
+2. ë¬¸ì œê°€ ì§€ì†ë˜ë©´ support@sajuos.comìœ¼ë¡œ ì—°ë½í•´ ì£¼ì„¸ìš”
+3. ê³§ ì •ìƒì ì¸ ë¶„ì„ ê²°ê³¼ë¡œ ì—…ë°ì´íŠ¸ë©ë‹ˆë‹¤
+
+---
+*ì´ ë©”ì‹œì§€ëŠ” ìë™ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤. ì˜¤ë¥˜ ì½”ë“œ: SECTION_EMPTY_{section_id}*
+"""
+                    content["body_markdown"] = fallback_text
+                    body_markdown = fallback_text
+                
+                # ğŸ”¥ P0: match_summaryë„ contentì— í¬í•¨
+                content["match_summary"] = match_summary
+                content["used_rulecard_ids"] = [c.get("id") for c in section_cards[:10]]
+                
+                await supabase_service.save_section(
+                    job_id=job_id,
+                    section_id=section_id,
+                    content_json=content
+                )
+                
+                sections_result[section_id] = content
+                
+                logger.info(f"[Worker] ì„¹ì…˜ ì™„ë£Œ: {section_id} | {len(body_markdown)}ì | ok={ok}")
+                
+                if not ok:
+                    failed_sections.append({"section_id": section_id, "errors": errors})
+                
+            except Exception as e:
+                logger.error(f"[Worker] ì„¹ì…˜ ì‹¤íŒ¨: {section_id} | {e}")
+                failed_sections.append({
+                    "section_id": section_id,
+                    "errors": [f"Exception: {str(e)[:100]}"]
+                })
+        
+        # ê²°ê³¼ JSON ìƒì„±
+        result_json = {
+            "name": name,
+            "target_year": target_year,
+            "saju_summary": {
+                "year_pillar": saju_data.get("year_pillar", ""),
+                "month_pillar": saju_data.get("month_pillar", ""),
+                "day_pillar": saju_data.get("day_pillar", ""),
+                "hour_pillar": saju_data.get("hour_pillar", ""),
+                "day_master": saju_data.get("day_master", ""),
+                "birth_info": saju_data.get("birth_info", ""),
+            },
+            # ğŸ”¥ P0: ì„¤ë¬¸ ë°ì´í„° ì €ì¥
+            "survey_data": survey_data,
+            "sections": sections_result,
+            "generated_at": time.strftime("%Y-%m-%d %H:%M:%S"),
+            "failed_sections": failed_sections if failed_sections else None,
+            # ğŸ”¥ P0: ì‚¬ìš©ëœ ë£°ì¹´ë“œ ID (ì „ì²´)
+            "top_used_rulecard_ids": all_used_card_ids[:20],
+            # ğŸ”¥ P0: ì„¹ì…˜ë³„ match_summary
+            "section_match_summaries": section_match_summaries,
+        }
+        
+        saju_json = {
+            "year_pillar": saju_data.get("year_pillar", ""),
+            "month_pillar": saju_data.get("month_pillar", ""),
+            "day_pillar": saju_data.get("day_pillar", ""),
+            "hour_pillar": saju_data.get("hour_pillar", ""),
+            "day_master": saju_data.get("day_master", ""),
+            "day_master_element": saju_data.get("day_master_element", ""),
+            "day_master_description": saju_data.get("day_master_description", ""),
+            "birth_info": saju_data.get("birth_info", ""),
+            "feature_tags": feature_tags,
+            "rulecards_used": all_used_card_ids[:20],
+            "survey_data": survey_data,
+            "calculated_at": time.strftime("%Y-%m-%d %H:%M:%S")
+        }
+        
+        markdown = self._build_markdown(result_json, saju_data)
+        
+        await supabase_service.complete_job(job_id, result_json, markdown, saju_json)
+        
+        logger.info(f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
+        logger.info(f"âœ… [Worker] Job ì™„ë£Œ: {job_id}")
+        logger.info(f"   ì‚¬ì£¼: {saju_json['year_pillar']}/{saju_json['month_pillar']}/{saju_json['day_pillar']}/{saju_json['hour_pillar']}")
+        logger.info(f"   ì„¤ë¬¸: {survey_data.get('industry', '-')} / {survey_data.get('painPoint', '-')}")
+        logger.info(f"   ì‚¬ìš© ì¹´ë“œ: {len(all_used_card_ids)}ê°œ")
+        logger.info(f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
+        
+        try:
+            await self._send_completion_email(email, name, job_id)
+        except Exception as e:
+            logger.warning(f"[Worker] ì™„ë£Œ ì´ë©”ì¼ ì‹¤íŒ¨: {e}")
+        
+        return True, ""
+    
+    def _get_all_cards_as_dict(self, rulestore: Any) -> List[Dict]:
+        """RuleStoreì—ì„œ ëª¨ë“  ì¹´ë“œë¥¼ dict ë¦¬ìŠ¤íŠ¸ë¡œ ì¶”ì¶œ"""
+        if not rulestore:
+            return []
+        
+        all_cards = getattr(rulestore, 'cards', [])
+        if not all_cards:
+            return []
+        
+        return [self._card_to_dict(c) for c in all_cards]
+    
+    def _select_rulecards_for_section(
+        self,
+        all_cards: List[Dict],
+        section_id: str,
+        feature_tags: List[str],
+        survey_data: Dict,
+        saju_data: Dict,
+        used_ids: set,
+    ) -> tuple[List[Dict], Dict]:
+        """
+        ğŸ”¥ğŸ”¥ğŸ”¥ P0 í•µì‹¬: RuleCardScorerë¥¼ ì‚¬ìš©í•˜ì—¬ ì„¤ë¬¸ ê¸°ë°˜ ì¹´ë“œ ì„ íƒ
+        
+        Returns:
+            (ì„ íƒëœ ì¹´ë“œ ë¦¬ìŠ¤íŠ¸, match_summary)
+        """
+        try:
+            from app.services.rulecard_scorer import rulecard_scorer
+            
+            # ğŸ”¥ P0: RuleCardScorer í˜¸ì¶œ - survey_data ì „ë‹¬!
+            section_cards = rulecard_scorer.score_cards_for_section(
+                all_cards=all_cards,
+                section_id=section_id,
+                feature_tags=feature_tags,
+                survey_data=survey_data,
+                existing_topics=set(),
+                saju_data=saju_data  # ğŸ”¥ P0: ì² ë²½ í•„í„°ë§ìš©
+            )
+            
+            # SectionCards ê°ì²´ì—ì„œ ë°ì´í„° ì¶”ì¶œ
+            selected_cards = []
+            for scored_card in section_cards.cards:
+                card_dict = {
+                    "id": scored_card.card_id,
+                    "topic": scored_card.topic,
+                    "subtopic": scored_card.subtopic,
+                    "score": scored_card.final_score,
+                    "matched_tags": scored_card.matched_tags,
+                    "score_trace": scored_card.score_trace.to_dict(),
+                }
+                # ì›ë³¸ ì¹´ë“œì—ì„œ ì¶”ê°€ í•„ë“œ ë³µì‚¬
+                for orig_card in all_cards:
+                    if orig_card.get("id") == scored_card.card_id:
+                        card_dict["trigger"] = orig_card.get("trigger", "")
+                        card_dict["mechanism"] = orig_card.get("mechanism", "")
+                        card_dict["interpretation"] = orig_card.get("interpretation", "")
+                        card_dict["action"] = orig_card.get("action", "")
+                        card_dict["cautions"] = orig_card.get("cautions", [])
+                        card_dict["tags"] = orig_card.get("tags", [])
+                        break
+                selected_cards.append(card_dict)
+            
+            match_summary = section_cards.match_summary
+            match_summary["avg_score"] = section_cards.avg_score
+            match_summary["total_selected"] = section_cards.total_cards
+            
+            return selected_cards, match_summary
+            
+        except Exception as e:
+            logger.exception(f"[Worker] RuleCardScorer í˜¸ì¶œ ì‹¤íŒ¨ - job FAILEDë¡œ ì²˜ë¦¬: {e}")
+            # ğŸ”¥ P0: Fallback ê¸ˆì§€, ì¦‰ì‹œ raise
+            raise RuntimeError(f"RuleCardScorer í˜¸ì¶œ ì‹¤íŒ¨: {e}") from e
+    
+    def _fallback_select_rulecards(self, all_cards: List[Dict], feature_tags: List[str]) -> tuple[List[Dict], Dict]:
+        """Fallback: RuleCardScorer ì‹¤íŒ¨ ì‹œ ë‹¨ìˆœ ì„ íƒ"""
+        if not all_cards:
+            return [], {"fallback": True, "reason": "no_cards"}
+        
+        if not feature_tags:
+            sorted_cards = sorted(all_cards, key=lambda c: c.get('priority', 0), reverse=True)
+            return sorted_cards[:50], {"fallback": True, "reason": "no_feature_tags"}
+        
+        matched = []
+        feature_set = set(t.lower() for t in feature_tags)
+        
+        for card in all_cards:
+            card_tags = card.get('tags', [])
+            card_tags_lower = set(t.lower() for t in card_tags)
+            if feature_set & card_tags_lower:
+                matched.append(card)
+        
+        if matched:
+            sorted_matched = sorted(matched, key=lambda c: c.get('priority', 0), reverse=True)
+            return sorted_matched[:50], {"fallback": True, "reason": "tag_match", "matched": len(matched)}
+        
+        sorted_cards = sorted(all_cards, key=lambda c: c.get('priority', 0), reverse=True)
+        return sorted_cards[:50], {"fallback": True, "reason": "priority_only"}
+    
+    async def _generate_section(
+        self,
+        section_id: str,
+        section_title: str,
+        saju_data: Dict,
+        rulecards: List,
+        feature_tags: List,
+        target_year: int,
+        question: str,
+        survey_data: Dict = None,
+        match_summary: Dict = None
+    ) -> Dict[str, Any]:
+        """ì„¹ì…˜ ìƒì„± - survey_data í¬í•¨"""
+        try:
+            from app.services.report_builder import premium_report_builder
+            
+            logger.info(f"[Worker:Section:{section_id}] ìƒì„± ì‹œì‘ | Cards={len(rulecards)}ì¥ | Title={section_title}")
+            
+            result = await premium_report_builder.regenerate_single_section(
+                section_id=section_id,
+                saju_data=saju_data,
+                rulecards=rulecards,
+                feature_tags=feature_tags,
+                target_year=target_year,
+                user_question=question,
+                survey_data=survey_data  # ğŸ”¥ P0: survey_data ì „ë‹¬
+            )
+            
+            if not result.get("success"):
+                error_msg = result.get("error", "Unknown error")
+                logger.error(f"[Worker:Section:{section_id}] ìƒì„± ì‹¤íŒ¨: {error_msg}")
+                return {
+                    "ok": False,
+                    "content": {"title": section_title, "body_markdown": "", "error": error_msg},
+                    "guardrail_errors": [error_msg]
+                }
+            
+            section_data = result.get("section", {})
+            body_markdown = section_data.get("body_markdown", "")
+            
+            if body_markdown:
+                logger.info(f"[Worker:Section:{section_id}] âœ… body_markdown={len(body_markdown)}ì")
+            else:
+                logger.warning(f"[Worker:Section:{section_id}] âš ï¸ body_markdown ë¹„ì–´ìˆìŒ!")
+            
+            content = {
+                "title": section_title,  # ğŸ”¥ P0: 1ì¸ ìì˜ì—…ììš© íƒ€ì´í‹€
+                "section_id": section_id,
+                "body_markdown": body_markdown,
+                "confidence": section_data.get("confidence", "MEDIUM"),
+                "diagnosis": section_data.get("diagnosis"),
+                "hypotheses": section_data.get("hypotheses"),
+                "strategy_options": section_data.get("strategy_options"),
+                "recommended_strategy": section_data.get("recommended_strategy"),
+                "kpis": section_data.get("kpis"),
+                "risks": section_data.get("risks"),
+                "annual_theme": section_data.get("annual_theme"),
+                "monthly_plans": section_data.get("monthly_plans"),
+                "quarterly_milestones": section_data.get("quarterly_milestones"),
+                "peak_months": section_data.get("peak_months"),
+                "risk_months": section_data.get("risk_months"),
+                "mission_statement": section_data.get("mission_statement"),
+                "phase_1_offer": section_data.get("phase_1_offer"),
+                "phase_2_funnel": section_data.get("phase_2_funnel"),
+                "phase_3_content": section_data.get("phase_3_content"),
+                "phase_4_automation": section_data.get("phase_4_automation"),
+                "milestones": section_data.get("milestones"),
+                "risk_scenarios": section_data.get("risk_scenarios"),
+            }
+            
+            return {
+                "ok": bool(body_markdown),
+                "content": content,
+                "guardrail_errors": [] if body_markdown else ["EMPTY_BODY_MARKDOWN"]
+            }
+            
+        except Exception as e:
+            logger.error(f"[Worker:Section:{section_id}] ì˜ˆì™¸: {e}")
+            return {
+                "ok": False,
+                "content": {"title": section_title, "body_markdown": "", "error": str(e)[:200]},
+                "guardrail_errors": [f"Exception: {str(e)[:100]}"]
+            }
+    
+    def _prepare_saju_data(self, input_json: Dict) -> Dict:
+        """ì‚¬ì£¼ ë°ì´í„° ì¶”ì¶œ"""
+        saju_result = input_json.get("saju_result") or {}
+        
+        def extract_ganji(pillar_data):
+            if not pillar_data:
+                return ""
+            if isinstance(pillar_data, dict):
+                return pillar_data.get("ganji", "")
+            if isinstance(pillar_data, str):
+                return pillar_data
+            return ""
+        
+        year_pillar = extract_ganji(saju_result.get("year_pillar"))
+        month_pillar = extract_ganji(saju_result.get("month_pillar"))
+        day_pillar = extract_ganji(saju_result.get("day_pillar"))
+        hour_pillar = extract_ganji(saju_result.get("hour_pillar"))
+        
+        saju_nested = saju_result.get("saju") or {}
+        if not year_pillar and saju_nested:
+            year_pillar = extract_ganji(saju_nested.get("year_pillar"))
+        if not month_pillar and saju_nested:
+            month_pillar = extract_ganji(saju_nested.get("month_pillar"))
+        if not day_pillar and saju_nested:
+            day_pillar = extract_ganji(saju_nested.get("day_pillar"))
+        if not hour_pillar and saju_nested:
+            hour_pillar = extract_ganji(saju_nested.get("hour_pillar"))
+        
+        if not year_pillar:
+            year_pillar = input_json.get("year_pillar", "")
+        if not month_pillar:
+            month_pillar = input_json.get("month_pillar", "")
+        if not day_pillar:
+            day_pillar = input_json.get("day_pillar", "")
+        if not hour_pillar:
+            hour_pillar = input_json.get("hour_pillar", "")
+        
+        day_master = saju_result.get("day_master", "")
+        if not day_master and saju_nested:
+            day_master = saju_nested.get("day_master", "")
+        
+        day_master_element = saju_result.get("day_master_element", "")
+        day_master_description = saju_result.get("day_master_description", "")
+        birth_info = saju_result.get("birth_info", {})
+        if isinstance(birth_info, str):
+            birth_info = {}
+        
+        # ğŸ”¥ P0: ëŒ€ìš´ ê³„ì‚° (ì„œë²„ í™•ì •ê°’)
+        survey_data = input_json.get("survey_data") or {}
+        gender = _normalize_gender(
+            input_json.get("gender") or 
+            birth_info.get("gender") or 
+            survey_data.get("gender") or 
+            saju_result.get("gender", "")
+        )
+        
+        age = _calc_age(birth_info)
+        if not age and birth_info.get("year"):
+            try:
+                age = date.today().year - int(birth_info.get("year"))
+            except:
+                age = 0
+        
+        year_stem = year_pillar[:1] if year_pillar else ""
+        
+        direction = None
+        daeun_list = []
+        current_daeun = None
+        
+        if gender and year_stem and month_pillar and age:
+            is_yang_year = _year_stem_is_yang(year_stem)
+            is_male = (gender == "male")
+            # ì–‘ë‚¨ìŒë…€=ìˆœí–‰, ìŒë‚¨ì–‘ë…€=ì—­í–‰
+            direction = "forward" if ((is_male and is_yang_year) or ((not is_male) and (not is_yang_year))) else "backward"
+            daeun_list = calc_daeun_pillars(month_pillar, direction, count=10)
+            if daeun_list:
+                start_age = int((saju_result.get('daeun_start_age') or saju_result.get('daeun_start') or saju_result.get('daeun_num') or input_json.get('daeun_start_age') or 3))
+                if not (1 <= start_age <= 10):
+                    start_age = 3  # fallback
+
+                idx = (age - start_age) // 10
+                if 0 <= idx < len(daeun_list):
+                    current_daeun = daeun_list[idx]
+            
+            logger.info(f"[Worker] ğŸ”¥ ëŒ€ìš´ ê³„ì‚°: gender={gender} | age={age} | direction={direction} | current_daeun={current_daeun}")
+        else:
+            logger.warning(f"[Worker] âš ï¸ ëŒ€ìš´ ê³„ì‚° ë¶ˆê°€: gender={gender} | year_stem={year_stem} | month_pillar={month_pillar} | age={age}")
+        
+        return {
+            "year_pillar": year_pillar,
+            "month_pillar": month_pillar,
+            "day_pillar": day_pillar,
+            "hour_pillar": hour_pillar,
+            "day_master": day_master,
+            "day_master_element": day_master_element,
+            "day_master_description": day_master_description,
+            "birth_info": birth_info,
+            "saju_result": saju_result,
+            # ğŸ”¥ P0: ëŒ€ìš´ ì •ë³´ ì¶”ê°€
+            "gender": gender,
+            "age": age,
+            "daeun_direction": direction,
+            "daeun_list": daeun_list,
+            "current_daeun": current_daeun,
+        }
+    
+    def _build_feature_tags(self, saju_data: Dict) -> List[str]:
+        """Feature Tags ìƒì„±"""
+        tags = []
+        
+        for pillar_key in ["year_pillar", "month_pillar", "day_pillar", "hour_pillar"]:
+            pillar = saju_data.get(pillar_key, "")
+            if pillar and len(pillar) >= 2:
+                tags.append(f"ì²œê°„:{pillar[0]}")
+                tags.append(f"ì§€ì§€:{pillar[1]}")
+        
+        if saju_data.get("day_master"):
+            tags.append(f"ì¼ê°„:{saju_data['day_master']}")
+        
+        return tags
+    
+    def _card_to_dict(self, card) -> Dict:
+        """RuleCardë¥¼ dictë¡œ ë³€í™˜ (content dict fallback í¬í•¨)"""
+        content = getattr(card, 'content', {}) or {}
+        return {
+            "id": getattr(card, 'id', ''),
+            "topic": getattr(card, 'topic', ''),
+            "subtopic": getattr(card, 'subtopic', '') or (getattr(card, 'meta', {}) or {}).get('subtopic', ''),
+            "tags": getattr(card, 'tags', []),
+            "priority": getattr(card, 'priority', 0),
+            "trigger": getattr(card, 'trigger', ''),
+            "mechanism": getattr(card, 'mechanism', '') or content.get('mechanism', ''),
+            "interpretation": getattr(card, 'interpretation', '') or content.get('interpretation', ''),
+            "action": getattr(card, 'action', '') or content.get('action', ''),
+            "cautions": getattr(card, 'cautions', []) or content.get('cautions', []),
+        }
+    
+    def _build_markdown(self, result_json: Dict, saju_data: Dict) -> str:
+        """ë§ˆí¬ë‹¤ìš´ ìƒì„±"""
+        lines = []
+        
+        name = result_json.get('name', 'ê³ ê°')
+        target_year = result_json.get('target_year', 2026)
+        survey_data = result_json.get('survey_data', {})
+        
+        lines.append(f"# {name}ë‹˜ì˜ {target_year}ë…„ 1ì¸ ì‚¬ì—…ê°€ ì „ëµ ë¦¬í¬íŠ¸\n")
+        
+        # ì„¤ë¬¸ ìš”ì•½
+        if survey_data:
+            lines.append("## ğŸ“‹ ë¹„ì¦ˆë‹ˆìŠ¤ í”„ë¡œí•„\n")
+            lines.append(f"- ì—…ì¢…: {survey_data.get('industry', '-')}")
+            lines.append(f"- ì›”ë§¤ì¶œ: {survey_data.get('revenue', '-')}")
+            lines.append(f"- í•µì‹¬ ë³‘ëª©: {survey_data.get('painPoint', '-')}")
+            lines.append(f"- 2026 ëª©í‘œ: {survey_data.get('goal', '-')}")
+            lines.append(f"- ì£¼ë‹¹ ì‹œê°„: {survey_data.get('time', '-')}")
+            lines.append("\n---\n")
+        
+        # ì‚¬ì£¼ ìš”ì•½
+        lines.append("## ğŸ“œ ì‚¬ì£¼ ì›êµ­\n")
+        lines.append(f"- ë…„ì£¼: {saju_data.get('year_pillar', '-')}")
+        lines.append(f"- ì›”ì£¼: {saju_data.get('month_pillar', '-')}")
+        lines.append(f"- ì¼ì£¼: {saju_data.get('day_pillar', '-')}")
+        lines.append(f"- ì‹œì£¼: {saju_data.get('hour_pillar', '-') or 'ë¯¸ì…ë ¥'}")
+        lines.append(f"- ì¼ê°„: {saju_data.get('day_master', '-')} ({saju_data.get('day_master_element', '')})")
+        lines.append("\n---\n")
+        
+        # ì„¹ì…˜ë³„ ë‚´ìš©
+        sections = result_json.get("sections", {})
+        for spec in ONEMAN_SECTION_SPECS:
+            section = sections.get(spec["id"], {})
+            lines.append(f"## {spec['title']}\n")
+            body = section.get("body_markdown", "") or section.get("summary", "ë‚´ìš© ì—†ìŒ")
+            lines.append(body)
+            lines.append("\n")
+        
+        return "\n".join(lines)
+    
+    async def _send_completion_email(self, email: str, name: str, job_id: str):
+        """ì™„ë£Œ ì´ë©”ì¼"""
+        if not email:
+            return
+        
+        try:
+            from app.services.email_sender import email_sender
+            
+            job = await supabase_service.get_job(job_id)
+            if not job:
+                return
+            
+            access_token = job.get("public_token", "")
+            if not access_token:
+                logger.error(f"[Worker] âš ï¸ public_tokenì´ NULL! job_id={job_id}")
+                return
+            
+            await email_sender.send_report_complete(
+                to_email=email,
+                name=name,
+                report_id=job_id,
+                access_token=access_token,
+                target_year=2026
+            )
+            logger.info(f"[Worker] âœ… ì™„ë£Œ ì´ë©”ì¼ ë°œì†¡: {email}")
+        except Exception as e:
+            logger.warning(f"ì´ë©”ì¼ ë°œì†¡ ì‹¤íŒ¨: {e}")
+    
+    async def _send_failure_email(self, job: Dict, error: str):
+        """ì‹¤íŒ¨ ì´ë©”ì¼"""
+        email = job.get("user_email", "")
+        if not email:
+            return
+        
+        try:
+            from app.services.email_sender import email_sender
+            input_json = job.get("input_json") or {}
+            name = input_json.get("name", "ê³ ê°")
+            job_id = job.get("id", "")
+            
+            await email_sender.send_report_failed(
+                to_email=email,
+                name=name,
+                report_id=job_id,
+                error_message=error[:200]
+            )
+            logger.info(f"[Worker] ì‹¤íŒ¨ ì´ë©”ì¼ ë°œì†¡: {email}")
+        except Exception as e:
+            logger.warning(f"ì‹¤íŒ¨ ì´ë©”ì¼ ë°œì†¡ ì‹¤íŒ¨: {e}")
+
+
+report_worker = ReportWorker()

diff --git a/app/services/rulecard_scorer.py b/app/services/rulecard_scorer.py
index 0000000..1111111 100644
--- a/app/services/rulecard_scorer.py
+++ b/app/services/rulecard_scorer.py
@@ -1,0 +1,662 @@
+"""
+RuleCard Scorer v2 - P0 Pivot: ì„¤ë¬¸ 5ë¬¸í•­ ê°€ì¤‘ì¹˜ + ìŠ¤ì½”ì–´ íŠ¸ë ˆì´ìŠ¤
+â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
+ğŸ”¥ P0 í•µì‹¬ ë³€ê²½:
+1. industry/painPoint/goal ì„¤ë¬¸ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ì¶”ê°€
+2. ê°™ì€ ì‚¬ì£¼ë¼ë„ ì„¤ë¬¸ì— ë”°ë¼ ì„ íƒ ì¹´ë“œê°€ ë‹¬ë¼ì§
+3. score_traceë¡œ ì ìˆ˜ breakdown ì œê³µ
+â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
+"""
+import logging
+from typing import Dict, Any, List, Set, Tuple, Optional
+from dataclasses import dataclass, field
+from collections import defaultdict
+import random
+
+logger = logging.getLogger(__name__)
+# --- P0 Fact-Check helpers (no-hallucination) ---
+_STEM_INFO = {
+    "ê°‘": ("wood", True), "ì„": ("wood", False),
+    "ë³‘": ("fire", True), "ì •": ("fire", False),
+    "ë¬´": ("earth", True), "ê¸°": ("earth", False),
+    "ê²½": ("metal", True), "ì‹ ": ("metal", False),
+    "ì„": ("water", True), "ê³„": ("water", False),
+}
+_BRANCH_ELEMENT = {
+    "ì":"water","ì¶•":"earth","ì¸":"wood","ë¬˜":"wood","ì§„":"earth","ì‚¬":"fire","ì˜¤":"fire","ë¯¸":"earth",
+    "ì‹ ":"metal","ìœ ":"metal","ìˆ ":"earth","í•´":"water"
+}
+# Element generating/controlling cycle
+_GEN = {"wood":"fire","fire":"earth","earth":"metal","metal":"water","water":"wood"}
+_CTRL = {"wood":"earth","earth":"water","water":"fire","fire":"metal","metal":"wood"}
+
+def _pillar_to_stem_branch(p: str):
+    if not p or len(p) < 2:
+        return None, None
+    return p[0], p[1]
+
+def _compute_fact_flags(saju_data: dict) -> dict:
+    """Compute presence flags from pillars + daeun for strict filtering."""
+    saju_data = saju_data or {}
+    dm = (saju_data.get("day_master") or "").strip()
+    dm_el = _STEM_INFO.get(dm, (None, None))[0] if dm else None
+
+    pillars = [saju_data.get(k) for k in ("year_pillar","month_pillar","day_pillar","hour_pillar")]
+    stems=set(); branches=set(); elements=set()
+    for p in pillars:
+        if not p: 
+            continue
+        s,b = _pillar_to_stem_branch(str(p).strip())
+        if s: stems.add(s); 
+        if b: branches.add(b)
+        if s in _STEM_INFO: elements.add(_STEM_INFO[s][0])
+        if b in _BRANCH_ELEMENT: elements.add(_BRANCH_ELEMENT[b])
+
+    # daeun
+    current_daeun = (saju_data.get("current_daeun") or "").strip()
+    d_stem, d_branch = _pillar_to_stem_branch(current_daeun) if current_daeun else (None,None)
+    daeun_elements=set()
+    if d_stem in _STEM_INFO: daeun_elements.add(_STEM_INFO[d_stem][0])
+    if d_branch in _BRANCH_ELEMENT: daeun_elements.add(_BRANCH_ELEMENT[d_branch])
+
+    def rel_element(kind: str):
+        if not dm_el:
+            return None
+        if kind == "wealth":   # ë‚´ê°€ ê·¹(controls)í•˜ëŠ” ì˜¤í–‰
+            return _CTRL.get(dm_el)
+        if kind == "output":   # ë‚´ê°€ ìƒ(creates)í•˜ëŠ” ì˜¤í–‰
+            return _GEN.get(dm_el)
+        if kind == "resource": # ë‚˜ë¥¼ ìƒí•˜ëŠ” ì˜¤í–‰
+            # inverse of GEN
+            inv = {v:k for k,v in _GEN.items()}
+            return inv.get(dm_el)
+        if kind == "power":    # ë‚˜ë¥¼ ê·¹í•˜ëŠ” ì˜¤í–‰
+            # inverse of CTRL
+            inv = {v:k for k,v in _CTRL.items()}
+            return inv.get(dm_el)
+        if kind == "peer":     # ë‚˜ì™€ ê°™ì€ ì˜¤í–‰
+            return dm_el
+        return None
+
+    wealth_el = rel_element("wealth")
+    output_el = rel_element("output")
+    resource_el = rel_element("resource")
+    power_el = rel_element("power")
+    peer_el = rel_element("peer")
+
+    flags = {
+        "day_master": dm,
+        "dm_element": dm_el,
+        "elements_present": elements,
+        "wealth_el": wealth_el,
+        "wealth_present": (wealth_el in elements) if wealth_el else None,
+        "wealth_in_daeun": (wealth_el in daeun_elements) if wealth_el else None,
+        "peer_present": (peer_el in elements) if peer_el else None,
+        "power_present": (power_el in elements) if power_el else None,
+        "output_present": (output_el in elements) if output_el else None,
+        "resource_present": (resource_el in elements) if resource_el else None,
+        "current_daeun": current_daeun,
+    }
+    return flags
+
+def _card_requires_flag(card_text: str):
+    ct = card_text
+    # map keywords -> required flag name
+    if any(k in ct for k in ["ì •ì¬","í¸ì¬","ì¬ì„±","ì¬ë¬¼","ê¸ˆì „ìš´","í˜„ê¸ˆíë¦„"]):
+        return "wealth"
+    if any(k in ct for k in ["ë¹„ê²¬","ê²ì¬","ë¹„ê²","ë™ì—…","íŒŒíŠ¸ë„ˆê°ˆë“±","í˜•ì œ"]):
+        return "peer"
+    if any(k in ct for k in ["ì •ê´€","í¸ê´€","ê´€ì„±","ëª…ì˜ˆ","ê·œìœ¨","ë²•","ê³„ì•½"]):
+        return "power"
+    if any(k in ct for k in ["ì‹ì‹ ","ìƒê´€","ì‹ìƒ","í‘œí˜„","ì˜ì—…","ì½˜í…ì¸ ","ë§ˆì¼€íŒ…"]):
+        return "output"
+    if any(k in ct for k in ["ì •ì¸","í¸ì¸","ì¸ì„±","ê³µë¶€","ìê²©","ë¬¸ì„œ","ì§€ì›"]):
+        return "resource"
+    return None
+
+
+
+# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
+# 1. ì‚¬ì—…ê°€í˜• í•µì‹¬ íƒœê·¸ 50 + ê°€ì¤‘ì¹˜
+# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
+
+BUSINESS_CORE_TAGS_50 = {
+    # â•â•â• ì¬ë¬¼/ê¸ˆì „ ê´€ë ¨ (15ê°œ) â•â•â•
+    "è²¡æ˜Ÿ": 10, "æ­£è²¡": 9, "åè²¡": 9, "è²¡åº«": 10, "ç ´è²¡": 8,
+    "æè²¡": 8, "è²¡é‹": 10, "æŠ•è³‡": 9, "æ”¶å…¥": 8, "æ”¯å‡º": 8,
+    "å¯Œè²´": 9, "è²¡å¤šèº«å¼±": 7, "è²¡æ—ºèº«å¼·": 9, "é£Ÿç¥ç”Ÿè²¡": 10, "åŠ«è²¡çˆ­è²¡": 6,
+    
+    # â•â•â• ì‚¬ì—…/ì»¤ë¦¬ì–´ ê´€ë ¨ (15ê°œ) â•â•â•
+    "å®˜æ˜Ÿ": 9, "æ­£å®˜": 8, "åå®˜": 8, "å°æ˜Ÿ": 9, "æ­£å°": 8,
+    "åå°": 8, "é£Ÿå‚·": 9, "é£Ÿç¥": 8, "å‚·å®˜": 8, "æ¯”åŠ«": 7,
+    "æ¯”è‚©": 7, "åŠ«è²¡": 7, "å‰µæ¥­": 10, "äº‹æ¥­": 10, "è½‰è·": 8,
+    
+    # â•â•â• ì‹œê¸°/íƒ€ì´ë° ê´€ë ¨ (10ê°œ) â•â•â•
+    "å¤§é‹": 10, "æµå¹´": 10, "æœˆé‹": 8, "å‰æ™‚": 9, "å‡¶æ™‚": 8,
+    "é–‹æ¥­": 9, "å‹•åœŸ": 7, "ç§»å¾™": 7, "åˆä½œ": 9, "è²´äººé‹": 10,
+    
+    # â•â•â• ê±´ê°•/ì—ë„ˆì§€ ê´€ë ¨ (5ê°œ) â•â•â•
+    "èº«å¼º": 9, "èº«å¼±": 8, "å¥åº·": 8, "å‹ç´¯": 7, "ç²¾ç¥": 7,
+    
+    # â•â•â• ê´€ê³„/ë„¤íŠ¸ì›Œí¬ ê´€ë ¨ (5ê°œ) â•â•â•
+    "è²´äºº": 10, "å°äºº": 7, "äººè„ˆ": 9, "åˆ": 8, "æ²–": 8,
+}
+
+# ì„¹ì…˜ë³„ ê°€ì¤‘ íƒœê·¸
+SECTION_TAG_WEIGHTS = {
+    "exec": {"å¤§é‹": 2.0, "æµå¹´": 2.0, "å‰æ™‚": 1.5, "è²´äººé‹": 1.5, "èº«å¼º": 1.5, "èº«å¼±": 1.5, "è²¡é‹": 1.5, "äº‹æ¥­": 1.5},
+    "money": {"è²¡æ˜Ÿ": 2.0, "æ­£è²¡": 2.0, "åè²¡": 2.0, "è²¡åº«": 2.0, "ç ´è²¡": 1.8, "æè²¡": 1.8, "æŠ•è³‡": 1.8, "æ”¶å…¥": 1.8, "é£Ÿç¥ç”Ÿè²¡": 2.0, "è²¡æ—ºèº«å¼·": 1.8, "è²¡å¤šèº«å¼±": 1.5},
+    "business": {"å‰µæ¥­": 2.0, "äº‹æ¥­": 2.0, "å®˜æ˜Ÿ": 1.8, "é£Ÿå‚·": 1.8, "å‚·å®˜": 1.5, "é£Ÿç¥": 1.5, "è½‰è·": 1.5, "åˆä½œ": 1.5},
+    "team": {"è²´äºº": 2.0, "äººè„ˆ": 2.0, "åˆ": 1.8, "æ²–": 1.5, "å°äºº": 1.5, "æ¯”åŠ«": 1.5, "æ¯”è‚©": 1.5, "åŠ«è²¡": 1.5},
+    "health": {"èº«å¼º": 2.0, "èº«å¼±": 2.0, "å¥åº·": 2.0, "å‹ç´¯": 1.8, "ç²¾ç¥": 1.8, "å°æ˜Ÿ": 1.5, "æ­£å°": 1.5},
+    "calendar": {"æœˆé‹": 2.0, "æµå¹´": 2.0, "å‰æ™‚": 2.0, "å‡¶æ™‚": 1.8, "é–‹æ¥­": 1.5, "å‹•åœŸ": 1.5, "ç§»å¾™": 1.5, "åˆä½œ": 1.5},
+    "sprint": {"å‰æ™‚": 2.0, "é–‹æ¥­": 2.0, "åˆä½œ": 1.8, "è²´äºº": 1.8, "è²¡é‹": 1.5, "äº‹æ¥­": 1.5, "è½‰è·": 1.5},
+}
+
+
+# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
+# 2. ğŸ”¥ P0: ì„¤ë¬¸ ê¸°ë°˜ ê°€ì¤‘ì¹˜ íƒœê·¸ ë§¤í•‘
+# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
+
+# ì—…ì¢… â†’ ê´€ë ¨ íƒœê·¸ + ê°€ì¤‘ì¹˜
+INDUSTRY_TAG_WEIGHTS: Dict[str, Dict[str, float]] = {
+    # IT/í…Œí¬
+    "it": {"å‰µæ¥­": 2.0, "äº‹æ¥­": 1.5, "é£Ÿå‚·": 1.8, "å‚·å®˜": 1.5, "å°æ˜Ÿ": 1.3},
+    "saas": {"å‰µæ¥­": 2.0, "äº‹æ¥­": 1.5, "é£Ÿå‚·": 1.8, "æ”¶å…¥": 2.0, "å‚·å®˜": 1.5},
+    "ê°œë°œ": {"å‰µæ¥­": 1.5, "å°æ˜Ÿ": 2.0, "é£Ÿå‚·": 1.8, "å‚·å®˜": 1.5},
+    "ai": {"å‰µæ¥­": 2.0, "å°æ˜Ÿ": 2.0, "é£Ÿå‚·": 1.8, "å‚·å®˜": 1.5},
+    "í”Œë«í¼": {"å‰µæ¥­": 2.0, "äº‹æ¥­": 2.0, "è²¡é‹": 1.8, "åˆä½œ": 1.5},
+    
+    # ì»¤ë¨¸ìŠ¤
+    "ì»¤ë¨¸ìŠ¤": {"è²¡æ˜Ÿ": 2.0, "æ­£è²¡": 2.0, "åè²¡": 1.8, "æŠ•è³‡": 1.5, "æ”¶å…¥": 2.0, "è²¡åº«": 1.5},
+    "ì‡¼í•‘ëª°": {"è²¡æ˜Ÿ": 2.0, "æ­£è²¡": 2.0, "åè²¡": 1.8, "æŠ•è³‡": 1.5, "æ”¶å…¥": 2.0},
+    "ì˜¨ë¼ì¸": {"è²¡æ˜Ÿ": 1.8, "æ­£è²¡": 1.8, "åè²¡": 1.5, "æ”¶å…¥": 1.8},
+    
+    # ì„œë¹„ìŠ¤
+    "ì»¨ì„¤íŒ…": {"å®˜æ˜Ÿ": 2.0, "æ­£å®˜": 1.8, "äººè„ˆ": 2.0, "è²´äºº": 1.8, "å°æ˜Ÿ": 1.5},
+    "êµìœ¡": {"å°æ˜Ÿ": 2.0, "æ­£å°": 2.0, "äººè„ˆ": 1.5, "é£Ÿç¥": 1.8},
+    "ì½”ì¹­": {"å°æ˜Ÿ": 2.0, "äººè„ˆ": 1.8, "é£Ÿç¥": 1.5, "è²´äºº": 1.5},
+    
+    # ìš”ì‹ì—…
+    "ì¹´í˜": {"è²¡æ˜Ÿ": 1.8, "é£Ÿç¥": 2.0, "æ”¶å…¥": 1.5, "å‹ç´¯": 1.5, "æŠ•è³‡": 1.3},
+    "ìŒì‹ì ": {"è²¡æ˜Ÿ": 1.8, "é£Ÿç¥": 2.0, "æ”¶å…¥": 1.5, "å‹ç´¯": 1.5},
+    "ì‹ë‹¹": {"è²¡æ˜Ÿ": 1.8, "é£Ÿç¥": 2.0, "æ”¶å…¥": 1.5, "å‹ç´¯": 1.5},
+    
+    # ì½˜í…ì¸ 
+    "ì½˜í…ì¸ ": {"é£Ÿå‚·": 2.0, "å‚·å®˜": 2.0, "é£Ÿç¥": 1.8, "å‰µæ¥­": 1.5, "æ”¶å…¥": 1.5},
+    "ìœ íŠœë¸Œ": {"é£Ÿå‚·": 2.0, "å‚·å®˜": 2.0, "äººè„ˆ": 1.8, "å‰µæ¥­": 1.5},
+    "í¬ë¦¬ì—ì´í„°": {"é£Ÿå‚·": 2.0, "å‚·å®˜": 2.0, "äººè„ˆ": 1.5},
+    
+    # ë¶€ë™ì‚°/íˆ¬ì
+    "ë¶€ë™ì‚°": {"è²¡æ˜Ÿ": 2.0, "æ­£è²¡": 2.0, "åè²¡": 2.0, "è²¡åº«": 2.0, "æŠ•è³‡": 2.0},
+    "íˆ¬ì": {"åè²¡": 2.0, "è²¡æ˜Ÿ": 2.0, "æŠ•è³‡": 2.0, "è²¡åº«": 1.8, "å¤§é‹": 1.5},
+}
+
+# ë³‘ëª© â†’ ê´€ë ¨ íƒœê·¸ + ê°€ì¤‘ì¹˜
+PAINPOINT_TAG_WEIGHTS: Dict[str, Dict[str, float]] = {
+    "lead": {"äººè„ˆ": 2.5, "è²´äºº": 2.0, "å®˜æ˜Ÿ": 1.5, "é£Ÿå‚·": 1.8, "å‚·å®˜": 1.5, "åˆä½œ": 1.5},
+    "conversion": {"è²¡æ˜Ÿ": 2.0, "æ­£è²¡": 2.0, "é£Ÿç¥ç”Ÿè²¡": 2.5, "åˆä½œ": 1.5, "å‰æ™‚": 1.5},
+    "operations": {"å°æ˜Ÿ": 2.0, "æ­£å°": 2.0, "å®˜æ˜Ÿ": 1.5, "å‹ç´¯": 1.8, "ç²¾ç¥": 1.5},
+    "funding": {"è²¡æ˜Ÿ": 2.5, "è²¡åº«": 2.5, "ç ´è²¡": 2.0, "æè²¡": 1.8, "åè²¡": 1.5, "æŠ•è³‡": 2.0},
+    "mental": {"èº«å¼±": 2.5, "å‹ç´¯": 2.5, "ç²¾ç¥": 2.0, "å¥åº·": 2.0, "å°æ˜Ÿ": 1.5},
+    "direction": {"å¤§é‹": 2.5, "æµå¹´": 2.0, "å®˜æ˜Ÿ": 1.8, "å°æ˜Ÿ": 1.5, "è½‰è·": 2.0},
+}
+
+# ëª©í‘œ í‚¤ì›Œë“œ â†’ ê´€ë ¨ íƒœê·¸ + ê°€ì¤‘ì¹˜
+GOAL_TAG_WEIGHTS: Dict[str, Dict[str, float]] = {
+    "ë§¤ì¶œ": {"è²¡æ˜Ÿ": 2.5, "æ­£è²¡": 2.0, "è²¡é‹": 2.0, "æ”¶å…¥": 2.0, "é£Ÿç¥ç”Ÿè²¡": 2.0},
+    "ìˆ˜ìµ": {"è²¡æ˜Ÿ": 2.5, "æ­£è²¡": 2.0, "è²¡é‹": 2.0, "æ”¶å…¥": 2.0},
+    "ëˆ": {"è²¡æ˜Ÿ": 2.5, "åè²¡": 2.0, "è²¡åº«": 2.0, "è²¡é‹": 2.0},
+    "ì›”ë§¤ì¶œ": {"è²¡æ˜Ÿ": 2.5, "æ­£è²¡": 2.0, "è²¡é‹": 2.0, "æ”¶å…¥": 2.0, "æœˆé‹": 1.5},
+    "í™•ì¥": {"å®˜æ˜Ÿ": 2.0, "äº‹æ¥­": 2.0, "åˆä½œ": 2.0, "æŠ•è³‡": 1.8, "å¤§é‹": 1.5},
+    "ìŠ¤ì¼€ì¼": {"å®˜æ˜Ÿ": 2.0, "äº‹æ¥­": 2.0, "åˆä½œ": 2.0, "æŠ•è³‡": 1.8},
+    "ì„±ì¥": {"å®˜æ˜Ÿ": 2.0, "äº‹æ¥­": 2.0, "å¤§é‹": 2.0, "æµå¹´": 1.5},
+    "íŒ€": {"æ¯”åŠ«": 2.0, "æ¯”è‚©": 2.0, "åˆä½œ": 2.5, "äººè„ˆ": 1.8, "å®˜æ˜Ÿ": 1.5},
+    "ì±„ìš©": {"æ¯”åŠ«": 2.0, "åˆä½œ": 2.0, "äººè„ˆ": 2.0, "å®˜æ˜Ÿ": 1.5},
+    "ë¸Œëœë“œ": {"å°æ˜Ÿ": 2.5, "æ­£å°": 2.0, "å®˜æ˜Ÿ": 1.8, "é£Ÿå‚·": 1.5},
+    "ì¸ì§€ë„": {"å°æ˜Ÿ": 2.0, "å®˜æ˜Ÿ": 2.0, "é£Ÿå‚·": 1.8, "äººè„ˆ": 1.5},
+    "ìë™í™”": {"å°æ˜Ÿ": 2.5, "æ­£å°": 2.0, "é£Ÿç¥": 1.8, "å®˜æ˜Ÿ": 1.5},
+    "ì‹œìŠ¤í…œ": {"å°æ˜Ÿ": 2.5, "æ­£å°": 2.0, "å®˜æ˜Ÿ": 1.5},
+    "ì•ˆì •": {"æ­£è²¡": 2.5, "è²¡åº«": 2.0, "èº«å¼º": 2.0, "å°æ˜Ÿ": 1.5},
+    "ì›Œë¼ë°¸": {"èº«å¼º": 2.5, "å¥åº·": 2.0, "ç²¾ç¥": 2.0, "å°æ˜Ÿ": 1.5},
+}
+
+
+# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
+# 3. ìŠ¤ì½”ì–´ë§ ê²°ê³¼ ë°ì´í„° êµ¬ì¡°
+# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
+
+@dataclass
+class ScoreTrace:
+    """ğŸ”¥ P0: ì ìˆ˜ breakdown (ë””ë²„ê¹…/íˆ¬ëª…ì„±)"""
+    priority: float = 0.0
+    tag_match: float = 0.0
+    section_bonus: float = 0.0
+    feature_match: float = 0.0
+    industry_match: float = 0.0
+    pain_match: float = 0.0
+    goal_match: float = 0.0
+    diversity_bonus: float = 0.0
+    
+    @property
+    def total(self) -> float:
+        return (
+            self.priority + self.tag_match + self.section_bonus +
+            self.feature_match + self.industry_match + self.pain_match +
+            self.goal_match + self.diversity_bonus
+        )
+    
+    def to_dict(self) -> Dict[str, float]:
+        return {
+            "priority": round(self.priority, 2),
+            "tag_match": round(self.tag_match, 2),
+            "section_bonus": round(self.section_bonus, 2),
+            "feature_match": round(self.feature_match, 2),
+            "industry_match": round(self.industry_match, 2),
+            "pain_match": round(self.pain_match, 2),
+            "goal_match": round(self.goal_match, 2),
+            "diversity_bonus": round(self.diversity_bonus, 2),
+            "total": round(self.total, 2),
+        }
+
+
+@dataclass
+class ScoredCard:
+    """ì ìˆ˜ê°€ ë§¤ê²¨ì§„ ë£°ì¹´ë“œ"""
+    card_id: str
+    topic: str
+    subtopic: str = ""
+    score: float = 0.0
+    matched_tags: List[str] = field(default_factory=list)
+    score_trace: ScoreTrace = field(default_factory=ScoreTrace)
+    
+    @property
+    def final_score(self) -> float:
+        return self.score_trace.total
+
+
+@dataclass 
+class SectionCards:
+    """ì„¹ì…˜ë³„ ì„ íƒëœ ì¹´ë“œë“¤"""
+    section_id: str
+    cards: List[ScoredCard]
+    total_cards: int
+    topic_distribution: Dict[str, int]
+    avg_score: float
+    # ğŸ”¥ P0: ë””ë²„ê¹…ìš© match_summary
+    match_summary: Dict[str, Any] = field(default_factory=dict)
+
+
+# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
+# 4. ğŸ”¥ P0: ì„¤ë¬¸ ê¸°ë°˜ ìŠ¤ì½”ì–´ë§ ì—”ì§„
+# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
+
+class RuleCardScorer:
+    """ì‚¬ì—…ê°€í˜• íƒœê·¸ + ì„¤ë¬¸ ê¸°ë°˜ ë£°ì¹´ë“œ ìŠ¤ì½”ì–´ë§"""
+    
+    def __init__(
+        self,
+        cards_per_section: int = 100,
+        min_diversity_ratio: float = 0.3,
+    ):
+        self.cards_per_section = cards_per_section
+        self.min_diversity_ratio = min_diversity_ratio
+    
+    def score_cards_for_section(
+        self,
+        all_cards: List[Dict[str, Any]],
+        section_id: str,
+        feature_tags: List[str],
+        survey_data: Optional[Dict[str, Any]] = None,
+        saju_data: Optional[Dict[str, Any]] = None,
+        existing_topics: Set[str] = None
+    ) -> SectionCards:
+        """
+        ğŸ”¥ P0: ì„¤ë¬¸ ê°€ì¤‘ì¹˜ ë°˜ì˜ ìŠ¤ì½”ì–´ë§
+        
+        Args:
+            all_cards: ì „ì²´ ë£°ì¹´ë“œ
+            section_id: ì„¹ì…˜ ID
+            feature_tags: ì‚¬ì£¼ ê¸°ë°˜ FeatureTags
+            survey_data: ğŸ”¥ P0 ì„¤ë¬¸ ë°ì´í„° (industry, painPoint, goal í¬í•¨)
+            existing_topics: ë‹¤ë¥¸ ì„¹ì…˜ì—ì„œ ì„ íƒëœ topicë“¤
+        """
+        existing_topics = existing_topics or set()
+        survey_data = survey_data or {}
+        fact_flags = _compute_fact_flags(saju_data or {})
+        
+        # ì„¹ì…˜ë³„ íƒœê·¸ ê°€ì¤‘ì¹˜
+        section_weights = SECTION_TAG_WEIGHTS.get(section_id, {})
+        
+        # ğŸ”¥ P0: ì„¤ë¬¸ ë°ì´í„° ì¶”ì¶œ
+        industry = (survey_data.get("industry") or "").lower()
+        pain_point = survey_data.get("painPoint") or survey_data.get("primary_bottleneck") or ""
+        goal = (survey_data.get("goal") or survey_data.get("goal_detail") or "").lower()
+        
+        # ğŸ”¥ P0: ì„¤ë¬¸ ê¸°ë°˜ ê°€ì¤‘ì¹˜ íƒœê·¸ ìˆ˜ì§‘
+        industry_weights = {}
+        for keyword, weights in INDUSTRY_TAG_WEIGHTS.items():
+            if keyword in industry:
+                for tag, weight in weights.items():
+                    industry_weights[tag] = max(industry_weights.get(tag, 0), weight)
+        
+        pain_weights = PAINPOINT_TAG_WEIGHTS.get(pain_point, {})
+        
+        goal_weights = {}
+        for keyword, weights in GOAL_TAG_WEIGHTS.items():
+            if keyword in goal:
+                for tag, weight in weights.items():
+                    goal_weights[tag] = max(goal_weights.get(tag, 0), weight)
+        
+        scored_cards: List[ScoredCard] = []
+        match_counts = {
+            "total": 0,
+            "industry_matched": 0,
+            "pain_matched": 0,
+            "goal_matched": 0,
+            "feature_matched": 0,
+            "section_matched": 0,
+        }
+        
+        for card in all_cards:
+            card_id = card.get("id", "")
+            topic = card.get("topic", "")
+            subtopic = card.get("subtopic", "")
+            card_tags = card.get("tags", [])
+            priority = card.get("priority", 0)
+            
+            if isinstance(card_tags, str):
+                card_tags = [card_tags]
+            
+            # P0 strict fact filter: do not score cards that contradict the user's pillars
+            card_text = " ".join([*(card_tags or []), str(card.get('title','')), str(card.get('content',''))])
+            req = _card_requires_flag(card_text)
+            if req == "wealth":
+                if fact_flags.get('wealth_present') is False and fact_flags.get('wealth_in_daeun') is False:
+                    continue  # ì¬ì„±/ì¬ë¬¼ ì¹´ë“œ ì°¨ë‹¨ (ì›êµ­+ëŒ€ìš´ ëª¨ë‘ ë¶€ì¬)
+            elif req == "peer":
+                if fact_flags.get('peer_present') is False:
+                    continue
+            elif req == "power":
+                if fact_flags.get('power_present') is False:
+                    continue
+            elif req == "output":
+                if fact_flags.get('output_present') is False:
+                    continue
+            elif req == "resource":
+                if fact_flags.get('resource_present') is False:
+                    continue
+
+            trace = ScoreTrace()
+            matched_tags = []
+            
+            # 1. Priority ì ìˆ˜
+            trace.priority = float(priority) * 0.5
+            
+            # 2. ê¸°ë³¸ ë¹„ì¦ˆë‹ˆìŠ¤ íƒœê·¸ ë§¤ì¹­
+            for tag in card_tags:
+                if tag in BUSINESS_CORE_TAGS_50:
+                    base_score = BUSINESS_CORE_TAGS_50[tag]
+                    
+                    # ì„¹ì…˜ë³„ ê°€ì¤‘ì¹˜ ì ìš©
+                    if tag in section_weights:
+                        base_score *= section_weights[tag]
+                        match_counts["section_matched"] += 1
+                    
+                    trace.tag_match += base_score
+                    matched_tags.append(tag)
+            
+            # 3. ğŸ”¥ğŸ”¥ğŸ”¥ P0 í•µì‹¬: FeatureTags ë§¤ì¹­ (ì‚¬ì£¼ ê¸°ë°˜) - ê°€ì¤‘ì¹˜ 10ë°° í­ë“±!
+            # ì‚¬ì£¼ ì›êµ­ê³¼ ë§ì§€ ì•ŠëŠ” ì¹´ë“œëŠ” ì ˆëŒ€ 1ë“±ì´ ë  ìˆ˜ ì—†ìŒ
+            for ft in feature_tags:
+                if ft.lower() in [t.lower() for t in card_tags]:
+                    trace.feature_match += 50.0  # ğŸ”¥ 5.0 â†’ 50.0 (10ë°° ì¦ê°€)
+                    match_counts["feature_matched"] += 1
+            
+            # 4. ğŸ”¥ P0: ì—…ì¢… ê°€ì¤‘ì¹˜
+            for tag in card_tags:
+                if tag in industry_weights:
+                    bonus = industry_weights[tag] * 3.0  # ì—…ì¢… ë§¤ì¹­ ë³´ë„ˆìŠ¤
+                    trace.industry_match += bonus
+                    if bonus > 0:
+                        match_counts["industry_matched"] += 1
+            
+            # 5. ğŸ”¥ P0: ë³‘ëª© ê°€ì¤‘ì¹˜
+            for tag in card_tags:
+                if tag in pain_weights:
+                    bonus = pain_weights[tag] * 3.0  # ë³‘ëª© ë§¤ì¹­ ë³´ë„ˆìŠ¤
+                    trace.pain_match += bonus
+                    if bonus > 0:
+                        match_counts["pain_matched"] += 1
+            
+            # 6. ğŸ”¥ P0: ëª©í‘œ ê°€ì¤‘ì¹˜
+            for tag in card_tags:
+                if tag in goal_weights:
+                    bonus = goal_weights[tag] * 3.0  # ëª©í‘œ ë§¤ì¹­ ë³´ë„ˆìŠ¤
+                    trace.goal_match += bonus
+                    if bonus > 0:
+                        match_counts["goal_matched"] += 1
+            
+            # 7. ë‹¤ì–‘ì„± ë³´ë„ˆìŠ¤
+            if topic and topic not in existing_topics:
+                trace.diversity_bonus = 3.0
+            
+            match_counts["total"] += 1
+            
+            scored_cards.append(ScoredCard(
+                card_id=card_id,
+                topic=topic,
+                subtopic=subtopic,
+                score=trace.total,
+                matched_tags=matched_tags,
+                score_trace=trace
+            ))
+        
+        # ì ìˆ˜ìˆœ ì •ë ¬
+        scored_cards.sort(key=lambda c: c.final_score, reverse=True)
+        
+        # ë‹¤ì–‘ì„± ë³´ì¥í•˜ë©´ì„œ Top-N ì„ íƒ
+        selected = self._select_with_diversity(scored_cards)
+        
+        # í†µê³„ ê³„ì‚°
+        topic_dist = defaultdict(int)
+        for card in selected:
+            topic_dist[card.topic] += 1
+        
+        avg_score = sum(c.score for c in selected) / len(selected) if selected else 0
+        
+        # ğŸ”¥ P0: match_summary ìƒì„±
+        match_summary = {
+            "section_id": section_id,
+            "total_cards": len(all_cards),
+            "selected_cards": len(selected),
+            "survey_applied": bool(industry or pain_point or goal),
+            "industry": industry,
+            "painPoint": pain_point,
+            "goal": goal[:50] if goal else "",
+            "match_counts": match_counts,
+            "top_5_cards": [
+                {
+                    "id": c.card_id,
+                    "score": round(c.final_score, 2),
+                    "trace": c.score_trace.to_dict()
+                }
+                for c in selected[:5]
+            ]
+        }
+        
+        logger.info(
+            f"[RuleCardScorer:{section_id}] "
+            f"Total={len(all_cards)} â†’ Selected={len(selected)} | "
+            f"Survey: industry={bool(industry)}, pain={bool(pain_point)}, goal={bool(goal)} | "
+            f"AvgScore={avg_score:.1f}"
+        )
+        
+        return SectionCards(
+            section_id=section_id,
+            cards=selected,
+            total_cards=len(selected),
+            topic_distribution=dict(topic_dist),
+            avg_score=avg_score,
+            match_summary=match_summary
+        )
+    
+    def _get_topic_relevance(self, topic: str, section_id: str) -> float:
+        """Topicê³¼ ì„¹ì…˜ ê°„ ê´€ë ¨ì„± ì ìˆ˜"""
+        section_topics = {
+            "exec": ["ìš´ì„¸", "ì¢…í•©", "ëŒ€ìš´", "ê¸¸í‰", "ì´ë¡ "],
+            "money": ["ì¬ë¬¼", "ì¬ìš´", "ê¸ˆì „", "íˆ¬ì", "ì¬ì •"],
+            "business": ["ì‚¬ì—…", "ì§ì—…", "ì»¤ë¦¬ì–´", "ì°½ì—…", "ì§„ë¡œ"],
+            "team": ["ì¸ê°„ê´€ê³„", "ëŒ€ì¸", "í˜‘ë ¥", "ê·€ì¸", "ì†Œì¸"],
+            "health": ["ê±´ê°•", "ì²´ë ¥", "ì—ë„ˆì§€", "ì»¨ë””ì…˜"],
+            "calendar": ["ì›”ìš´", "ì¼ì§„", "ì‹œê¸°", "ë‚ ì§œ"],
+            "sprint": ["ì‹¤í–‰", "ê³„íš", "ì•¡ì…˜", "ë‹¨ê¸°"],
+        }
+        
+        relevant_topics = section_topics.get(section_id, [])
+        for rel_topic in relevant_topics:
+            if rel_topic in topic:
+                return 5.0
+        return 0.0
+    
+    def _select_with_diversity(self, scored_cards: List[ScoredCard]) -> List[ScoredCard]:
+        """ë‹¤ì–‘ì„±ì„ ë³´ì¥í•˜ë©´ì„œ Top-N ì„ íƒ"""
+        if not scored_cards:
+            return []
+        
+        target_count = min(self.cards_per_section, len(scored_cards))
+        top_half = int(target_count * 0.5)
+        
+        # ìƒìœ„ 50%ëŠ” ì ìˆ˜ìˆœ
+        selected = scored_cards[:top_half]
+        used_topics = {c.topic for c in selected}
+        
+        # ë‚˜ë¨¸ì§€ëŠ” ë‹¤ì–‘ì„± ê³ ë ¤
+        remaining = scored_cards[top_half:]
+        
+        by_topic: Dict[str, List[ScoredCard]] = defaultdict(list)
+        for card in remaining:
+            by_topic[card.topic].append(card)
+        
+        unused_topics = [t for t in by_topic.keys() if t not in used_topics]
+        used_topic_list = list(used_topics & set(by_topic.keys()))
+        topic_order = unused_topics + used_topic_list
+        
+        while len(selected) < target_count:
+            added_any = False
+            for topic in topic_order:
+                if len(selected) >= target_count:
+                    break
+                if by_topic[topic]:
+                    card = by_topic[topic].pop(0)
+                    selected.append(card)
+                    added_any = True
+            if not added_any:
+                break
+        
+        return selected
+    
+    def score_all_sections(
+        self,
+        all_cards: List[Dict[str, Any]],
+        feature_tags: List[str],
+        survey_data: Optional[Dict[str, Any]] = None,
+        section_ids: List[str] = None
+    ) -> Dict[str, SectionCards]:
+        """ëª¨ë“  ì„¹ì…˜ì— ëŒ€í•´ ìŠ¤ì½”ì–´ë§"""
+        if section_ids is None:
+            section_ids = ["exec", "money", "business", "team", "health", "calendar", "sprint"]
+        
+        results = {}
+        used_topics: Set[str] = set()
+        
+        for section_id in section_ids:
+            section_cards = self.score_cards_for_section(
+                all_cards=all_cards,
+                section_id=section_id,
+                feature_tags=feature_tags,
+                survey_data=survey_data,
+                existing_topics=used_topics
+            )
+            results[section_id] = section_cards
+            used_topics.update(section_cards.topic_distribution.keys())
+        
+        return results
+    
+    def get_cards_for_prompt(
+        self,
+        section_cards: SectionCards,
+        max_chars: int = 8000
+    ) -> str:
+        """í”„ë¡¬í”„íŠ¸ì— ì£¼ì…í•  ë£°ì¹´ë“œ í…ìŠ¤íŠ¸ ìƒì„±"""
+        lines = [
+            f"=== {section_cards.section_id.upper()} ì„¹ì…˜ ê´€ë ¨ RuleCards ({section_cards.total_cards}ì¥) ===",
+            f"í‰ê·  ê´€ë ¨ë„ ì ìˆ˜: {section_cards.avg_score:.1f}",
+            f"Topic ë¶„í¬: {dict(section_cards.topic_distribution)}",
+            "",
+        ]
+        
+        current_len = sum(len(l) for l in lines)
+        
+        for card in section_cards.cards:
+            card_text = f"[{card.card_id}] ({card.topic}/{card.subtopic}) ì ìˆ˜:{card.score:.1f} íƒœê·¸:{','.join(card.matched_tags[:5])}"
+            
+            if current_len + len(card_text) > max_chars:
+                lines.append(f"... ì™¸ {len(section_cards.cards) - len(lines) + 4}ì¥ (ë¬¸ì ì œí•œìœ¼ë¡œ ìƒëµ)")
+                break
+            
+            lines.append(card_text)
+            current_len += len(card_text)
+        
+        return "\n".join(lines)
+
+
+# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
+# 5. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
+# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
+
+def get_business_core_tags() -> Dict[str, int]:
+    """ì‚¬ì—…ê°€í˜• í•µì‹¬ íƒœê·¸ 50 ì¡°íšŒ"""
+    return BUSINESS_CORE_TAGS_50.copy()
+
+
+def get_section_tag_weights(section_id: str) -> Dict[str, float]:
+    """ì„¹ì…˜ë³„ íƒœê·¸ ê°€ì¤‘ì¹˜ ì¡°íšŒ"""
+    return SECTION_TAG_WEIGHTS.get(section_id, {}).copy()
+
+
+def get_survey_tag_weights(survey_data: Dict[str, Any]) -> Dict[str, Dict[str, float]]:
+    """
+    ğŸ”¥ P0: ì„¤ë¬¸ ë°ì´í„°ì—ì„œ ì¶”ì¶œí•œ ê°€ì¤‘ì¹˜ íƒœê·¸ ì¡°íšŒ
+    """
+    result = {
+        "industry_weights": {},
+        "pain_weights": {},
+        "goal_weights": {},
+    }
+    
+    industry = (survey_data.get("industry") or "").lower()
+    pain_point = survey_data.get("painPoint") or survey_data.get("primary_bottleneck") or ""
+    goal = (survey_data.get("goal") or survey_data.get("goal_detail") or "").lower()
+    
+    for keyword, weights in INDUSTRY_TAG_WEIGHTS.items():
+        if keyword in industry:
+            for tag, weight in weights.items():
+                result["industry_weights"][tag] = max(
+                    result["industry_weights"].get(tag, 0), weight
+                )
+    
+    result["pain_weights"] = PAINPOINT_TAG_WEIGHTS.get(pain_point, {})
+    
+    for keyword, weights in GOAL_TAG_WEIGHTS.items():
+        if keyword in goal:
+            for tag, weight in weights.items():
+                result["goal_weights"][tag] = max(
+                    result["goal_weights"].get(tag, 0), weight
+                )
+    
+    return result
+
+
+# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
+rulecard_scorer = RuleCardScorer()

diff --git a/app/services/report_builder.py b/app/services/report_builder.py
index 0000000..1111111 100644
--- a/app/services/report_builder.py
+++ b/app/services/report_builder.py
@@ -1,0 +1,568 @@
+"""
+SajuOS Premium Report Builder v12 - P0 ë¹ˆ ì„¹ì…˜ ì ˆëŒ€ ê¸ˆì§€
+â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
+ğŸ”¥ P0-1: ì¹´ë“œ 0ê°œ â†’ LLM í˜¸ì¶œ X, í´ë°± í…ìŠ¤íŠ¸ ì¦‰ì‹œ ë°˜í™˜
+ğŸ”¥ P0-2: ì„¹ì…˜ ID ì •í•©ì„± (exec,money,business,team,health,calendar,sprint)
+ğŸ”¥ P0-3: í† í° "ì¹˜í™˜" (ì‚­ì œ X) - {industry}â†’"í•´ë‹¹ ì—…ì¢…"
+ğŸ”¥ P0-4: ìƒì„± ì‹¤íŒ¨ ì›ì¸ ë¡œê·¸ 4ê°œ í•„ìˆ˜
+â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
+"""
+import asyncio
+import logging
+import re
+from typing import Dict, Any, List
+from dataclasses import dataclass, field
+
+from openai import AsyncOpenAI
+import httpx
+
+from app.config import get_settings
+from app.services.openai_key import get_openai_api_key
+from app.services.terminology_mapper import sanitize_for_business
+from app.services.job_store import job_store
+from app.templates.master_samples import load_master_samples, get_master_body_markdown
+
+logger = logging.getLogger(__name__)
+
+MASTER_SAMPLES = load_master_samples("v1")
+
+DEBUG_TEMPLATE_LEAKS = False
+
+
+# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
+# P0-3: í† í° ì¹˜í™˜ (ì‚­ì œ X)
+# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
+
+TOKEN_REPLACEMENTS = {
+    "{industry}": "í•´ë‹¹ ì—…ì¢…",
+    "{painPoint}": "í˜„ì¬ ë³‘ëª©",
+    "{engine_headline}": "í•µì‹¬ ê²°ë¡ ",
+    "{goal}": "ëª©í‘œ",
+    "{revenue}": "ë§¤ì¶œ",
+    "{day_master}": "ì¼ê°„",
+    "{time}": "ì‹œì ",
+    "[ENGINE_HEADLINE]": "",
+    "[/ENGINE_HEADLINE]": "",
+}
+
+
+def replace_template_tokens(text: str) -> str:
+    """ğŸ”¥ P0-3: í† í° ì¹˜í™˜ (ì‚­ì œê°€ ì•„ë‹Œ ì˜ë¯¸ ìˆëŠ” í…ìŠ¤íŠ¸ë¡œ ëŒ€ì²´)"""
+    if not text:
+        return ""
+    if DEBUG_TEMPLATE_LEAKS:
+        return text.strip()
+    for token, replacement in TOKEN_REPLACEMENTS.items():
+        text = text.replace(token, replacement)
+    text = re.sub(r"\{[a-zA-Z_]+\}", "í•´ë‹¹ í•­ëª©", text)
+    return text.strip()
+
+
+def check_template_leaks(text: str, context: str = "") -> List[str]:
+    if not text:
+        return []
+    leaked = []
+    for token in TOKEN_REPLACEMENTS.keys():
+        if token in text:
+            leaked.append(token)
+    if re.search(r"\{[a-zA-Z_]+\}", text):
+        leaked.append("{other}")
+    if leaked:
+        logger.warning(f"[TemplateLeak] {context} | leaked: {leaked}")
+    return leaked
+
+
+# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
+# P0-2: ì„¹ì…˜ ID ì •í•©ì„± (ê¸°ì¡´ ID ìœ ì§€)
+# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
+
+@dataclass
+class SectionSpec:
+    id: str
+    title: str
+    max_cards: int
+    min_chars: int
+    fallback_headline: str
+    topic_filter: List[str] = field(default_factory=list)
+
+
+# ğŸ”¥ P0-2: í•©ì˜ëœ section_id ê³ ì • (exec, money, business, team, health, calendar, sprint)
+PREMIUM_SECTIONS = {
+    "exec": SectionSpec(
+        "exec", "2026 ë¹„ì¦ˆë‹ˆìŠ¤ ì „ëµ ê¸°ìƒë„", 20, 1500,
+        "í˜„ì¬ ì‚¬ì£¼ êµ¬ì¡°ìƒ 2026ë…„ ë¹„ì¦ˆë‹ˆìŠ¤ í™˜ê²½ì€ ë³€í™”ì˜ ê¸°ìš´ì´ ê°ì§€ë©ë‹ˆë‹¤",
+        topic_filter=["ì „ì²´ìš´", "ì¢…í•©", "ì¼ê°„", "ì„±í–¥", "ê¸°ìš´", "ìš´ì„¸"]
+    ),
+    "money": SectionSpec(
+        "money", "ìë³¸ ìœ ë™ì„± ë° í˜„ê¸ˆíë¦„ ìµœì í™”", 20, 2500,
+        "í˜„ì¬ êµ¬ì¡°ìƒ í˜„ê¸ˆíë¦„ì˜ ë³€ë™ì„±ì´ ì˜ˆìƒë©ë‹ˆë‹¤",
+        topic_filter=["ì¬ë¬¼", "ì¬ì„±", "ì •ì¬", "í¸ì¬", "í˜„ê¸ˆ", "ë§¤ì¶œ", "íˆ¬ì"]
+    ),
+    "business": SectionSpec(
+        "business", "ì‹œì¥ í¬ì§€ì…”ë‹ ë° ìƒí’ˆ í™•ì¥ ì „ëµ", 20, 2500,
+        "í˜„ì¬ êµ¬ì¡°ìƒ ì‹œì¥ í¬ì§€ì…”ë‹ ì¬ê²€í† ê°€ í•„ìš”í•©ë‹ˆë‹¤",
+        topic_filter=["ì‚¬ì—…", "ì°½ì—…", "ê²½ì˜", "ê´€ì„±", "ì •ê´€", "í¸ê´€", "ì‹œì¥"]
+    ),
+    "team": SectionSpec(
+        "team", "ì¡°ì§ í™•ì¥ ë° íŒŒíŠ¸ë„ˆì‹­ ê°€ì´ë“œ", 20, 2000,
+        "í˜„ì¬ êµ¬ì¡°ìƒ íŒŒíŠ¸ë„ˆì‹­ ê´€ë¦¬ê°€ í•µì‹¬ ê³¼ì œì…ë‹ˆë‹¤",
+        topic_filter=["ë¹„ê²", "ë¹„ê²¬", "ê²ì¬", "ë™ì—…", "íŒŒíŠ¸ë„ˆ", "í˜‘ë ¥", "ì¸ë§¥"]
+    ),
+    "health": SectionSpec(
+        "health", "ì£¼ìš” ì¥ì• ë¬¼ ë° ë¦¬ìŠ¤í¬ (2026)", 15, 1500,
+        "í˜„ì¬ êµ¬ì¡°ìƒ í•´ë‹¹ ë¦¬ìŠ¤í¬ëŠ” ë‚®ì€ ìˆ˜ì¤€ì…ë‹ˆë‹¤",
+        topic_filter=["ë¦¬ìŠ¤í¬", "ìœ„í—˜", "ì¶©", "í˜•", "íŒŒ", "ì†í•´", "ì¥ì• ", "ë²ˆì•„ì›ƒ"]
+    ),
+    "calendar": SectionSpec(
+        "calendar", "12ê°œì›” ë¹„ì¦ˆë‹ˆìŠ¤ ìŠ¤í”„ë¦°íŠ¸ ìº˜ë¦°ë”", 15, 2500,
+        "í˜„ì¬ êµ¬ì¡°ìƒ ì›”ë³„ ë¦¬ë“¬ì— ë§ì¶˜ ì „ëµì´ í•„ìš”í•©ë‹ˆë‹¤",
+        topic_filter=["ì›”ìš´", "ì‹œê¸°", "ê³„ì ˆ", "íƒ€ì´ë°", "ê¸¸ì¼", "í‰ì¼", "ëŒ€ìš´"]
+    ),
+    "sprint": SectionSpec(
+        "sprint", "í–¥í›„ 90ì¼ ë§¤ì¶œ ê·¹ëŒ€í™” ì•¡ì…˜í”Œëœ", 15, 2000,
+        "í˜„ì¬ êµ¬ì¡°ìƒ 90ì¼ ì§‘ì¤‘ ì‹¤í–‰ì´ íš¨ê³¼ì ì…ë‹ˆë‹¤",
+        topic_filter=["ì‹¤í–‰", "ì•¡ì…˜", "ê³„íš", "ëª©í‘œ", "ì‹ì‹ ", "ìƒê´€", "ì‹ìƒ"]
+    ),
+}
+
+
+# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
+# P0-1: í´ë°± í…ìŠ¤íŠ¸ (ë¹ˆ ì„¹ì…˜ ë°©ì§€)
+# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
+
+def generate_fallback_body(section_id: str, engine_headline: str, survey_data: Dict = None) -> str:
+    """ğŸ”¥ P0-1: ì¹´ë“œ 0ê°œ ë˜ëŠ” LLM ì‹¤íŒ¨ ì‹œ í´ë°± í…ìŠ¤íŠ¸"""
+    spec = PREMIUM_SECTIONS.get(section_id)
+    if not spec:
+        spec = SectionSpec(section_id, "ì„¹ì…˜", 10, 500, "ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤")
+    
+    headline = engine_headline if engine_headline else spec.fallback_headline
+    industry = (survey_data or {}).get("industry", "í•´ë‹¹ ì—…ì¢…")
+    painPoint = (survey_data or {}).get("painPoint", "í˜„ì¬ ë³‘ëª©")
+    
+    return f"""{headline}
+
+## í˜„ì¬ ìƒí™© ë¶„ì„
+
+ì›ì¸(ì‚¬ì£¼/ë£°ì¹´ë“œ) ì •ë³´ê°€ ì¶©ë¶„í•˜ì§€ ì•Šì•„ ìƒì„¸ ë¶„ì„ì´ ì œí•œë©ë‹ˆë‹¤.
+ì„¤ë¬¸ìœ¼ë¡œë§Œ ì–µì§€ ì¶”ë¡ í•˜ëŠ” ê²ƒì€ Root Cause Rule ìœ„ë°˜ì´ë¯€ë¡œ ìƒëµí•©ë‹ˆë‹¤.
+
+### ë‹¤ìŒ í–‰ë™ ê¶Œì¥ì‚¬í•­
+
+1. **D+14**: {industry} ì—…ì¢… í˜„í™© ì ê²€ ë° ë°ì´í„° ìˆ˜ì§‘
+2. **D+30**: {painPoint} ê´€ë ¨ í•µì‹¬ ì§€í‘œ ëª¨ë‹ˆí„°ë§ ì‹œì‘
+3. **D+60**: ìˆ˜ì§‘ëœ ë°ì´í„° ê¸°ë°˜ ì „ëµ ì¬ìˆ˜ë¦½
+
+### ì²´í¬ë¦¬ìŠ¤íŠ¸
+- [ ] í˜„ì¬ ìƒí™© ê°ê´€ì  ì§„ë‹¨
+- [ ] í•µì‹¬ ì§€í‘œ ì •ì˜
+- [ ] ë°ì´í„° ìˆ˜ì§‘ ì²´ê³„ êµ¬ì¶•
+- [ ] ì£¼ê°„ ë¦¬ë·° ì¼ì • í™•ì •
+- [ ] ì „ë¬¸ê°€ ìƒë‹´ ê²€í† 
+
+---
+*ì¶”ê°€ ì‚¬ì£¼ ì •ë³´ë‚˜ ë£°ì¹´ë“œ ë§¤ì¹­ì´ í™•ë³´ë˜ë©´ ë” ì •ë°€í•œ ë¶„ì„ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.*
+"""
+
+
+# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
+# ë°ì´í„° êµ¬ì¡°
+# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
+
+@dataclass
+class SectionRuleCardAllocation:
+    section_id: str
+    allocated_count: int
+    allocated_card_ids: List[str]
+    context_text: str
+    cards: List[Dict[str, Any]] = field(default_factory=list)
+
+
+def score_card_for_section(card: Dict, section_id: str, survey_data: Dict = None) -> float:
+    spec = PREMIUM_SECTIONS.get(section_id)
+    if not spec:
+        return 1.0
+    score = 1.0
+    topic = (card.get("topic") or "").lower()
+    mechanism = (card.get("mechanism") or "").lower()
+    tags = " ".join(card.get("tags") or []).lower()
+    card_text = f"{topic} {mechanism} {tags}"
+    for tf in spec.topic_filter:
+        if tf.lower() in card_text:
+            score += 3.0
+    if survey_data:
+        pain = (survey_data.get("painPoint") or "").lower()
+        pain_tags = {"lead": ["ì¸ë§¥", "ê·€ì¸"], "retention": ["ë¹„ê²", "ë¹„ê²¬"], "conversion": ["ì¬ì„±", "ì •ì¬"], "funding": ["ì¬ì„±", "íˆ¬ì"]}
+        for tag in pain_tags.get(pain, []):
+            if tag.lower() in card_text:
+                score += 2.0
+    return score
+
+
+def allocate_rulecards_to_section(all_cards: List[Dict], section_id: str, max_cards: int, used_ids: set, survey_data: Dict = None) -> SectionRuleCardAllocation:
+    scored = []
+    for card in all_cards:
+        cid = card.get("id", card.get("_id", ""))
+        if cid in used_ids:
+            continue
+        score = score_card_for_section(card, section_id, survey_data)
+        scored.append((score, card))
+    scored.sort(key=lambda x: x[0], reverse=True)
+    
+    filtered = [(s, c) for s, c in scored if s > 1.0]
+    if not filtered:
+        logger.warning(f"[CardAlloc] section={section_id} topic_filter hit=0 â†’ fallback")
+        if scored:
+            filtered = scored[:max_cards]
+        elif all_cards:
+            fallback = [c for c in all_cards if c.get("id", c.get("_id", "")) not in used_ids][:max_cards]
+            filtered = [(1.0, c) for c in fallback]
+    
+    allocated = [card for _, card in filtered[:max_cards]]
+    ids, lines = [], []
+    for card in allocated:
+        cid = card.get("id", card.get("_id", ""))
+        ids.append(cid)
+        interp = sanitize_for_business((card.get("interpretation") or "")[:200])
+        lines.append(f"[{cid}] {card.get('topic', '')} | {interp}")
+    
+    logger.info(f"[CardAlloc] section={section_id} | scored={len(scored)} | filtered={len(filtered)} | allocated={len(ids)}")
+    return SectionRuleCardAllocation(section_id, len(ids), ids, "\n".join(lines), allocated)
+
+
+def extract_engine_headline(cards: List[Dict]) -> str:
+    if not cards:
+        return ""
+    top_card = cards[0]
+    interp = top_card.get("interpretation") or top_card.get("content", {}).get("interpretation", "") or top_card.get("mechanism") or ""
+    interp = sanitize_for_business(interp)
+    sentences = re.split(r"[.ã€‚!?]", interp)
+    first = sentences[0].strip() if sentences else interp[:100]
+    first = re.sub(r"\{[a-zA-Z_]+\}", "", first)
+    return first if first else interp[:100]
+
+
+# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
+# í”„ë¡¬í”„íŠ¸
+# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
+
+ROOT_CAUSE_RULE = """## ğŸ§  Root Cause Rule (ì ˆëŒ€ê·œì¹™)
+- ì‚¬ì£¼/ë£°ì¹´ë“œ(=ì›ì¸)ê°€ ê²°ë¡ ì´ë‹¤. ì„¤ë¬¸(=ì¦ìƒ)ì€ ê²°ë¡ ì´ ì•„ë‹ˆë‹¤.
+- ì„¹ì…˜ì˜ ì²« ë¬¸ì¥ì€ ë°˜ë“œì‹œ ì—”ì§„ì´ í™•ì •í•œ ê²°ë¡ ìœ¼ë¡œ ì‹œì‘í•œë‹¤.
+- ê¸ˆì§€: "ê³ ê°ë‹˜ì´ ì„¤ë¬¸ì—ì„œ ~ë¼ê³  í•˜ì…¨ìœ¼ë‹ˆ" ê°™ì€ ì„œìˆ .
+"""
+
+
+
+
+TENGOD_ORDER = ["ë¹„ê²¬","ê²ì¬","ì‹ì‹ ","ìƒê´€","í¸ì¬","ì •ì¬","í¸ê´€","ì •ê´€","í¸ì¸","ì •ì¸"]
+
+
+def build_fact_check_context(saju_data: Dict[str, Any]) -> str:
+    yp = saju_data.get("year_pillar","")
+    mp = saju_data.get("month_pillar","")
+    dp = saju_data.get("day_pillar","")
+    hp = saju_data.get("hour_pillar","")
+    dm = saju_data.get("day_master","")
+    gender = saju_data.get("gender","")
+    age = saju_data.get("age",0)
+    cur = saju_data.get("current_daeun","")
+    direction = saju_data.get("daeun_direction","")
+    tg = saju_data.get("ten_gods_present") or []
+    dtg = saju_data.get("daeun_ten_gods") or []
+    elems = saju_data.get("elements_present") or []
+    has_wealth = bool(saju_data.get("has_wealth_star"))
+
+    def _fmt(xs, order=None):
+        if not xs:
+            return "(ì—†ìŒ)"
+        if order:
+            xs = [x for x in order if x in set(xs)] + [x for x in xs if x not in set(order)]
+        return ", ".join(xs)
+
+    return (
+        "## ğŸš¨ ì›êµ­ íŒ©íŠ¸ì²´í¬ (ì ˆëŒ€ ì¤€ìˆ˜)\n"
+        f"- ì›êµ­(4ì£¼): {yp} {mp} {dp} {hp}\n"
+        f"- ì¼ê°„: {dm}\n"
+        f"- ì„±ë³„/ë§Œë‚˜ì´: {gender} / {age}\n"
+        f"- í˜„ì¬ ëŒ€ìš´: {cur} (ë°©í–¥={direction})\n"
+        f"- ì›êµ­ ì‹­ì„±(ì²œê°„+ì§€ì¥ê°„): {_fmt(tg, TENGOD_ORDER)}\n"
+        f"- í˜„ì¬ëŒ€ìš´ ì‹­ì„±: {_fmt(dtg, TENGOD_ORDER)}\n"
+        f"- ì˜¤í–‰: {_fmt(elems)}\n"
+        f"- ì¬ì„±(ì •ì¬/í¸ì¬) ì›êµ­ ì¡´ì¬: {'ìˆìŒ' if has_wealth else 'ì—†ìŒ'}\n\n"
+        "### ê¸ˆì§€ ê·œì¹™\n"
+        "1) ìœ„ 'ì›êµ­ ì‹­ì„±'ì— ì—†ëŠ” ì‹­ì„±ì„ 'ìˆë‹¤'ê³  ë‹¨ì •í•˜ì§€ ë§ˆë¼.\n"
+        "2) ì¬ì„±ì´ ì›êµ­ì— ì—†ìœ¼ë©´ 'ì •ì¬/í¸ì¬ê°€ ìˆë‹¤'ë¼ê³  ë§í•˜ì§€ ë§ˆë¼.\n"
+        "3) ëŒ€ìš´ ë³€í™”ëŠ” ë°˜ë“œì‹œ 'ëŒ€ìš´ì—ì„œ ë“¤ì–´ì˜¨ë‹¤'ë¡œ ì›êµ­ê³¼ êµ¬ë¶„í•´ì„œ ë§í•´ë¼.\n"
+    )
+def build_system_prompt(section_id: str, engine_headline: str, survey_data: Dict = None, saju_data: Dict = None, existing_contents: List[str] = None, cards_summary: str = "") -> str:
+    spec = PREMIUM_SECTIONS.get(section_id)
+    if not spec:
+        logger.error(f"[Builder] Invalid section_id: {section_id}")
+        return ""
+    title = spec.title
+    min_chars = spec.min_chars
+    master_body = get_master_body_markdown(section_id)
+    industry = (survey_data or {}).get("industry", "") or "ë¯¸ì…ë ¥"
+    painPoint = (survey_data or {}).get("painPoint", "") or "ë¯¸ì…ë ¥"
+    businessGoal = (survey_data or {}).get("businessGoal", "") or "ë¯¸ì…ë ¥"
+    survey_context = f"\n## ì„¤ë¬¸ (ì¦ìƒ)\n- ì—…ì¢…: {industry}\n- ê³ ë¯¼: {painPoint}\n- ëª©í‘œ: {businessGoal}\n"
+    existing_block = ""
+    if existing_contents:
+        existing_block = f"\n## ì´ì „ ì„¹ì…˜ (ë°˜ë³µ ê¸ˆì§€)\n{chr(10).join(existing_contents[-2:])}\n"
+    
+    # ğŸ”¥ P0: ì›êµ­ íŒ©íŠ¸ ì²´í¬ ë¸”ë¡ ì¶”ê°€
+    fact_ctx = build_fact_check_context(saju_data or {})
+    
+    return f"""ë„ˆëŠ” [{title}] ì „ë¬¸ ì»¨ì„¤í„´íŠ¸ë‹¤.
+
+{ROOT_CAUSE_RULE}
+{fact_ctx}
+
+## ì²« ë¬¸ì¥ (ìˆ˜ì • ê¸ˆì§€)
+"{engine_headline}"
+
+## ë§ˆìŠ¤í„° ìƒ˜í”Œ
+{master_body if master_body else '(ììœ  ì‘ì„±)'}
+
+## ë£°ì¹´ë“œ
+{cards_summary if cards_summary else '(ì—†ìŒ)'}
+{survey_context}
+{existing_block}
+
+## ê·œì¹™
+1) ì²« ë¬¸ì¥: ìœ„ ì—”ì§„ ê²°ë¡ ìœ¼ë¡œ ì‹œì‘
+2) ë¦¬ìŠ¤í¬ 2ê°œ, ì•¡ì…˜ 3ê°œ, ì²´í¬ë¦¬ìŠ¤íŠ¸ 7ê°œ
+3) ìµœì†Œ {min_chars}ì, í•œêµ­ì–´ë¡œë§Œ
+"""
+
+
+# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
+# ë¹Œë” í´ë˜ìŠ¤
+# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
+
+class PremiumReportBuilder:
+    def __init__(self):
+        self._client = None
+        self._semaphore = None
+    
+    def _get_client(self) -> AsyncOpenAI:
+        api_key = get_openai_api_key()
+        return AsyncOpenAI(api_key=api_key, timeout=httpx.Timeout(120.0, connect=15.0), max_retries=2)
+    
+    async def build_premium_report(self, saju_data: Dict, rulecards: List[Dict], feature_tags: List[str] = None, target_year: int = 2026, user_question: str = "", name: str = "ê³ ê°", job_id: str = None, survey_data: Dict = None, mode: str = "premium"):
+        self._semaphore = asyncio.Semaphore(2)
+        self._client = self._get_client()
+        if job_id:
+            await job_store.start_job(job_id)
+        
+        used_card_ids = set()
+        results = []
+        existing_contents = []
+        
+        for sid in PREMIUM_SECTIONS.keys():
+            spec = PREMIUM_SECTIONS[sid]
+            alloc = allocate_rulecards_to_section(rulecards, sid, spec.max_cards, used_card_ids, survey_data)
+            used_card_ids.update(alloc.allocated_card_ids)
+            engine_headline = extract_engine_headline(alloc.cards)
+            
+            # ğŸ”¥ P0-4: ìƒì„± ì‹¤íŒ¨ ì›ì¸ ë¡œê·¸ 4ê°œ í•„ìˆ˜
+            headline_len = len(engine_headline) if engine_headline else 0
+            logger.info(f"[Builder] ğŸ“Š section={sid} | 1.allocated_count={alloc.allocated_count} | 2.headline_len={headline_len}")
+            
+            try:
+                result = await self._generate_section_safe(
+                    section_id=sid,
+                    saju_data=saju_data,
+                    allocation=alloc,
+                    target_year=target_year,
+                    survey_data=survey_data,
+                    engine_headline=engine_headline,
+                    existing_contents=existing_contents,
+                    job_id=job_id
+                )
+                
+                body = result.get("body_markdown", "")
+                body_len = len(body)
+                
+                # ğŸ”¥ P0-4: LLM ì‘ë‹µ ê¸¸ì´ + ìµœì¢… ì €ì¥ ê¸¸ì´ ë¡œê·¸
+                logger.info(f"[Builder] ğŸ“Š section={sid} | 3.llm_response_len={result.get('llm_response_len', 0)} | 4.final_body_len={body_len}")
+                
+                if body_len == 0:
+                    logger.error(f"[Builder] âŒ section={sid} | generated_len=0 â†’ EMPTY SECTION")
+                elif body_len < 200:
+                    logger.warning(f"[Builder] âš ï¸ section={sid} | generated_len={body_len} < 200 â†’ TOO SHORT")
+                else:
+                    logger.info(f"[Builder] âœ… section={sid} | generated_len={body_len}")
+                
+                if body:
+                    existing_contents.append(body[:300])
+                results.append(result)
+                
+                if job_id:
+                    await job_store.section_done(job_id, sid, body_len)
+                    
+            except Exception as e:
+                logger.exception(f"[Builder] âŒ ì„¹ì…˜ ìƒì„± ì‹¤íŒ¨: {sid} | {e}")
+                # ğŸ”¥ P0-1: ì˜ˆì™¸ ì‹œì—ë„ í´ë°±ìœ¼ë¡œ ë¹ˆ ì„¹ì…˜ ë°©ì§€
+                fallback_body = generate_fallback_body(sid, engine_headline, survey_data)
+                result = {
+                    "section_id": sid,
+                    "title": spec.title,
+                    "body_markdown": fallback_body,
+                    "engine_headline": engine_headline or spec.fallback_headline,
+                    "rulecard_ids": [],
+                    "char_count": len(fallback_body),
+                    "is_fallback": True,
+                    "error": str(e)[:200]
+                }
+                results.append(result)
+                logger.warning(f"[Builder] ğŸ”„ section={sid} | fallback_len={len(fallback_body)}")
+        
+        if job_id:
+            await job_store.complete_job(job_id, {"sections": len(results)})
+        return {"status": "success", "sections": results}
+    
+    async def _generate_section_safe(self, section_id: str, saju_data: Dict, allocation: SectionRuleCardAllocation, target_year: int, survey_data: Dict, engine_headline: str, existing_contents: List[str], job_id: str = None) -> Dict[str, Any]:
+        """ğŸ”¥ P0-1: ë¹ˆ ì„¹ì…˜ ì ˆëŒ€ ê¸ˆì§€ - ì¹´ë“œ 0ê°œë©´ í´ë°±"""
+        spec = PREMIUM_SECTIONS.get(section_id)
+        if not spec:
+            logger.error(f"[Builder] Invalid section_id: {section_id}")
+            raise ValueError(f"Invalid section_id: {section_id}")
+        
+        # ğŸ”¥ P0-1(A): ì¹´ë“œ 0ê°œë©´ LLM í˜¸ì¶œ X, ì¦‰ì‹œ í´ë°±
+        if allocation.allocated_count == 0:
+            logger.warning(f"[Builder] section={section_id} | cards=0 â†’ skip LLM, use fallback")
+            fallback_body = generate_fallback_body(section_id, engine_headline, survey_data)
+            return {
+                "section_id": section_id,
+                "title": spec.title,
+                "body_markdown": fallback_body,
+                "engine_headline": engine_headline or spec.fallback_headline,
+                "rulecard_ids": [],
+                "char_count": len(fallback_body),
+                "llm_response_len": 0,
+                "is_fallback": True
+            }
+        
+        cards_summary = self._build_cards_summary(allocation.cards[:5])
+        system_prompt = build_system_prompt(
+            section_id=section_id,
+            engine_headline=engine_headline or spec.fallback_headline,
+            survey_data=survey_data,
+            saju_data=saju_data,  # ğŸ”¥ P0: íŒ©íŠ¸ì²´í¬ìš©
+            existing_contents=existing_contents,
+            cards_summary=cards_summary
+        )
+        user_prompt = self._build_user_prompt(saju_data, allocation, target_year)
+        
+        llm_response_len = 0
+        body_markdown = ""
+        
+        async with self._semaphore:
+            try:
+                response = await self._client.chat.completions.create(
+                    model="gpt-4o",
+                    messages=[
+                        {"role": "system", "content": system_prompt},
+                        {"role": "user", "content": user_prompt}
+                    ],
+                    temperature=0.7,
+                    max_tokens=4000
+                )
+                body_markdown = response.choices[0].message.content or ""
+                llm_response_len = len(body_markdown)
+            except Exception as e:
+                logger.error(f"[Builder] GPT í˜¸ì¶œ ì‹¤íŒ¨: {section_id} | {e}")
+                # ğŸ”¥ P0-1(B): ì˜ˆì™¸ ì‹œ í´ë°±
+                body_markdown = generate_fallback_body(section_id, engine_headline, survey_data)
+                llm_response_len = 0
+        
+        # LLM ì‘ë‹µì´ ë„ˆë¬´ ì§§ìœ¼ë©´ í´ë°±
+        if len(body_markdown) < 200:
+            logger.warning(f"[Builder] section={section_id} | llm_response too short ({len(body_markdown)}) â†’ fallback")
+            body_markdown = generate_fallback_body(section_id, engine_headline, survey_data)
+        
+        body_markdown = self._enforce_engine_headline(body_markdown, engine_headline or spec.fallback_headline)
+        
+        # ğŸ”¥ P0-3: leak ì²´í¬ í›„ ì¹˜í™˜
+        leaked = check_template_leaks(body_markdown, f"section={section_id}")
+        body_markdown = replace_template_tokens(body_markdown)
+        
+        return {
+            "section_id": section_id,
+            "title": spec.title,
+            "body_markdown": body_markdown,
+            "engine_headline": engine_headline or spec.fallback_headline,
+            "rulecard_ids": allocation.allocated_card_ids,
+            "char_count": len(body_markdown),
+            "llm_response_len": llm_response_len,
+            "leaked_tokens": leaked
+        }
+    
+    def _build_cards_summary(self, cards: List[Dict]) -> str:
+        lines = []
+        for i, c in enumerate(cards[:5], 1):
+            interp = (c.get("interpretation") or "")[:80]
+            lines.append(f"{i}. [{c.get('topic', '')}] {interp}")
+        return "\n".join(lines) if lines else "(ì—†ìŒ)"
+    
+    def _build_user_prompt(self, saju_data: Dict, allocation: SectionRuleCardAllocation, target_year: int) -> str:
+        year_pillar = saju_data.get("year_pillar", "-")
+        month_pillar = saju_data.get("month_pillar", "-")
+        day_pillar = saju_data.get("day_pillar", "-")
+        hour_pillar = saju_data.get("hour_pillar", "-") or "ë¯¸ì…ë ¥"
+        day_master = saju_data.get("day_master", "")
+        card_lines = []
+        for c in allocation.cards[:10]:
+            interp = (c.get("interpretation") or "")[:100]
+            card_lines.append(f"- [{c.get('id', '')}] {c.get('topic', '')} | {interp}")
+        return f"""## ì‚¬ì£¼ ì›êµ­
+| ë…„ì£¼ | ì›”ì£¼ | ì¼ì£¼ | ì‹œì£¼ |
+|------|------|------|------|
+| {year_pillar} | {month_pillar} | {day_pillar} | {hour_pillar} |
+
+- ì¼ê°„: {day_master}
+- ë¶„ì„ë…„ë„: {target_year}ë…„
+
+## ë£°ì¹´ë“œ
+{chr(10).join(card_lines) if card_lines else '(ì—†ìŒ)'}
+
+ìœ„ ì •ë³´ë¡œ ì‘ì„±í•˜ì„¸ìš”.
+"""
+    
+    def _enforce_engine_headline(self, body_markdown: str, engine_headline: str) -> str:
+        if not engine_headline:
+            return body_markdown
+        headline = engine_headline.strip()
+        body_stripped = body_markdown.lstrip()
+        if body_stripped.startswith(headline):
+            return body_markdown
+        if len(body_stripped) > 50 and headline[:30] in body_stripped[:100]:
+            return body_markdown
+        logger.warning(f"[Builder] engine_headline ê°•ì œ ì‚½ì…")
+        return f"{headline}\n\n{body_stripped}"
+    
+    async def regenerate_single_section(self, section_id: str, saju_data: Dict, rulecards: List[Dict], feature_tags: List[str] = None, target_year: int = 2026, user_question: str = "", survey_data: Dict = None):
+        self._client = self._get_client()
+        self._semaphore = asyncio.Semaphore(1)
+        spec = PREMIUM_SECTIONS.get(section_id)
+        if not spec:
+            logger.error(f"[Builder] Invalid section_id: {section_id}")
+            raise ValueError(f"Invalid section_id: {section_id}")
+        alloc = allocate_rulecards_to_section(rulecards, section_id, spec.max_cards, set(), survey_data)
+        engine_headline = extract_engine_headline(alloc.cards)
+        result = await self._generate_section_safe(
+            section_id=section_id,
+            saju_data=saju_data,
+            allocation=alloc,
+            target_year=target_year,
+            survey_data=survey_data,
+            engine_headline=engine_headline,
+            existing_contents=[]
+        )
+        return {"success": True, "section": result}
+
+
+premium_report_builder = PremiumReportBuilder()
+report_builder = premium_report_builder
